{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, id: int, class_: str | None, abstract: str) -> None:\n",
    "        self.id: int = id\n",
    "        self.class_: str = class_\n",
    "        self.abstract: str = abstract\n",
    "\n",
    "    def get_type(self) -> str:\n",
    "        '''Returns: \"test\" | \"train\"'''\n",
    "        return \"train\" if self.class_ else \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModel Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModel:\n",
    "    def __init__(self, data_path: str) -> None:\n",
    "        self.data: list[Data] = []\n",
    "        self.vocabulary_size: int = 0\n",
    "        if not os.path.exists(data_path):\n",
    "            self.data = []\n",
    "        else:\n",
    "            with open(data_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            for line in lines[1:]:\n",
    "                line_split = line.strip().replace('\"', '').split(',')\n",
    "                if len(line_split) == 3:\n",
    "                    id, class_, abstract = line_split\n",
    "                    data = Data(id, class_, abstract)\n",
    "                    self.data.append(data)\n",
    "                else:\n",
    "                    id, abstract = line_split\n",
    "                    data = Data(id, None, abstract)\n",
    "                    self.data.append(data)\n",
    "        self.vocabulary_size = self.get_vocabulary_size()\n",
    "    \n",
    "\n",
    "    def get_vocabulary_size(self) -> int:\n",
    "        '''Returns the number of unique words in the dataset.'''\n",
    "        vocabulary = set()\n",
    "        for data in self.data:\n",
    "            words = data.abstract.split()\n",
    "            for word in words:\n",
    "                vocabulary.add(word)\n",
    "        self.vocabulary_size = len(vocabulary)\n",
    "        return self.vocabulary_size\n",
    "\n",
    "\n",
    "    def split_model(self, proportion: float) -> 'DataModel':\n",
    "        '''Splits this dataset into two data sets based on the proportion of the current data to be in the new split.'''\n",
    "        split_data = DataModel('')\n",
    "        # random.shuffle(self.data)\n",
    "        split_index = int(len(self.data) * proportion)\n",
    "        split_data.data = self.data[:split_index]\n",
    "        self.data = self.data[split_index:]\n",
    "        self.vocabulary_size = self.get_vocabulary_size()\n",
    "        return split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardNaiveBayes:\n",
    "    def __init__(self) -> None:\n",
    "        self.training_data: DataModel = DataModel(\n",
    "            data_path=os.path.join(os.path.join(\"data\", \"trg.csv\"))\n",
    "        )\n",
    "        self.testing_data: DataModel = DataModel(\n",
    "            data_path=os.path.join(os.path.join(\"data\", \"tst.csv\"))\n",
    "        )\n",
    "        self.classes = [c for c in set([data.class_ for data in self.training_data.data])]\n",
    "        self.class_counts = self.get_class_counts()\n",
    "        self.class_probabilities = [count / len(self.training_data.data) for count in self.class_counts]\n",
    "        self.word_counts = self.get_word_counts()\n",
    "\n",
    "\n",
    "    def run_test_data(self, fileout: str, type_: str = \"test\") -> None:\n",
    "        with open(fileout, 'w') as f:\n",
    "            f.write(\"id,class\\n\")\n",
    "            if type_ == \"test\":\n",
    "                for data in self.testing_data.data:\n",
    "                    f.write(f\"{data.id},{self.classify_abstract(data.abstract)}\\n\")\n",
    "            elif type_ == \"train\":\n",
    "                for data in self.training_data.data:\n",
    "                    f.write(f\"{data.id},{self.classify_abstract(data.abstract)}\\n\")\n",
    "            else:\n",
    "                raise ValueError(\"Invalid type_ argument. Must be 'test', 'validation', or 'train'.\")\n",
    "\n",
    "\n",
    "    def get_word_probability(self, word: str, class_index: int) -> float:\n",
    "        '''\n",
    "        p(class|word) = p(word|class) * p(class) / p(word)\n",
    "        '''\n",
    "        word_count = self.word_counts[class_index].get(word, 0)\n",
    "        class_count = self.class_counts[class_index]\n",
    "        word_in_class = word_count / class_count\n",
    "        class_probability = self.class_probabilities[class_index]\n",
    "        word_in_data = sum([self.word_counts[i].get(word, 0) for i in range(len(self.classes))]) / len(self.training_data.data)\n",
    "        if word_in_data == 0:\n",
    "            return 0\n",
    "        return word_in_class * class_probability / word_in_data\n",
    "\n",
    "\n",
    "    def test_classify_abstract(self, abstract: str) -> str:\n",
    "        abstract_words = abstract.split()\n",
    "        class_probabilities = []\n",
    "        for i in range(len(self.classes)):\n",
    "            cur_class_probability = self.class_probabilities[i]\n",
    "            cur_class_probability = 1\n",
    "            for word in abstract_words:\n",
    "                cur_word_probability = self.get_word_probability(word, i)\n",
    "                if cur_word_probability == 0:\n",
    "                    continue\n",
    "                cur_class_probability *= cur_word_probability\n",
    "                print(f\"WORD: {word} | CLASS: {self.classes[i]} | PROB: {cur_class_probability}\")\n",
    "            class_probabilities.append(cur_class_probability)\n",
    "        max_class = self.classes[class_probabilities.index(max(class_probabilities))]\n",
    "        print(f\"Final Probabilities: {class_probabilities}\")\n",
    "        return max_class\n",
    "\n",
    "\n",
    "    def classify_abstract(self, abstract: str) -> str:\n",
    "        '''Classifies the abstract into one of the classes. Returns the class. Uses the Naive Bayesian Classifier algorithm.'''\n",
    "        abstract_words = abstract.split()\n",
    "        class_probabilities = []\n",
    "        for i in range(len(self.classes)):\n",
    "            cur_class_probability = self.class_probabilities[i]\n",
    "            cur_class_probability = 1\n",
    "            for word in abstract_words:\n",
    "                cur_word_probability = self.get_word_probability(word, i)\n",
    "                if cur_word_probability == 0:\n",
    "                    continue\n",
    "                cur_class_probability *= cur_word_probability\n",
    "            class_probabilities.append(cur_class_probability)\n",
    "        max_class = self.classes[class_probabilities.index(max(class_probabilities))]\n",
    "        return max_class\n",
    "\n",
    "\n",
    "    def get_class_counts(self) -> list[int]:\n",
    "        '''Returns the count of each class in the training data. Match the order of the classes with the order of self.classes (classes[i] -> class_counts[i])'''\n",
    "        class_counts = [0] * len(self.classes)\n",
    "        for data in self.training_data.data:\n",
    "            class_counts[self.classes.index(data.class_)] += 1\n",
    "        return class_counts\n",
    "    \n",
    "\n",
    "    def get_word_counts(self) -> list[dict[str, int]]:\n",
    "        '''Returns the count of each word in each class. Match the order of the classes with the order of self.classes (classes[i] -> word_counts[i])'''\n",
    "        word_counts = [{} for _ in range(len(self.classes))]\n",
    "        for data in self.training_data.data:\n",
    "            for word in data.abstract.split():\n",
    "                if word not in word_counts[self.classes.index(data.class_)]:\n",
    "                    word_counts[self.classes.index(data.class_)][word] = 1\n",
    "                else:\n",
    "                    word_counts[self.classes.index(data.class_)][word] += 1\n",
    "        return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    '''\n",
    "\n",
    "    classifier = StandardNaiveBayes()\n",
    "    classifier.run_test_data(\"output.csv\", type_=\"test\")\n",
    "\n",
    "    -->    Accuracy Obtained: 0.800 (Kaggle)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of Standard Naive Bayesian Classifier\n",
    "...\n",
    "\n",
    "# Improvements \n",
    "...\n",
    "- Add-1 LaPlace Smoothing\n",
    "- Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedNaiveBayes:\n",
    "    def __init__(self, validation_data_split: float, alpha: float) -> None:\n",
    "        self.training_data: DataModel = DataModel(\n",
    "            data_path=os.path.join(os.path.join(\"data\", \"trg.csv\"))\n",
    "        )\n",
    "        self.testing_data: DataModel = DataModel(\n",
    "            data_path=os.path.join(os.path.join(\"data\", \"tst.csv\"))\n",
    "        )\n",
    "        self.validation_data: DataModel = self.training_data.split_model(validation_data_split)\n",
    "        print(f\"Size of training data: {len(self.training_data.data)}\")\n",
    "        print(f\"Size of validation data: {len(self.validation_data.data)}\")\n",
    "        print(f\"Size of testing data: {len(self.testing_data.data)}\")\n",
    "        print(f\"Vocab size of training data: {self.training_data.vocabulary_size}\")\n",
    "\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.classes = [c for c in set([data.class_ for data in self.training_data.data])]\n",
    "        self.class_counts = self.get_class_counts()\n",
    "        self.word_counts = self.get_word_counts() # Change: Implemented Add-One Laplace Smoothing\n",
    "        # self.word_counts: list[dict[str, int]] = [{} for _ in range(len(self.classes))]\n",
    "        # self.class_counts: list[int] = [0] * len(self.classes)\n",
    "        self.vocab_size = 0\n",
    "        # self.estimate_parameters()\n",
    "        self.class_probabilities = [count / len(self.training_data.data) for count in self.class_counts]\n",
    "\n",
    "\n",
    "    def estimate_parameters(self) -> None:\n",
    "        for data in self.training_data.data:\n",
    "            class_index = self.classes.index(data.class_)\n",
    "            self.class_counts[class_index] += 1\n",
    "            for word in data.abstract.split():\n",
    "                # Update word counts for each class\n",
    "                self.word_counts[class_index][word] = self.word_counts[class_index].get(word, 0) + 1\n",
    "                # Update vocabulary size if word is totally new (for all classes)\n",
    "                for i in range(len(self.classes)):\n",
    "                    if word not in self.word_counts[i]:\n",
    "                        self.vocab_size += 1\n",
    "                        i = len(self.classes) # Break the loop\n",
    "\n",
    "        # Smoothing and parameter calculation\n",
    "        for class_index in range(len(self.classes)):\n",
    "            class_total_with_smoothing = self.class_counts[class_index] + self.alpha * self.vocab_size\n",
    "            for word in self.word_counts[class_index]:\n",
    "                self.word_counts[class_index][word] = (self.word_counts[class_index][word] + self.alpha) / class_total_with_smoothing\n",
    "\n",
    "\n",
    "    def get_validation_accuracy(self) -> float:\n",
    "        correct = 0\n",
    "        for data in self.validation_data.data:\n",
    "            predicted_class = self.classify_abstract(data.abstract)\n",
    "            if predicted_class == data.class_:\n",
    "                correct += 1\n",
    "        return correct / len(self.validation_data.data)\n",
    "\n",
    "\n",
    "    def run_test_data(self, fileout: str, type_: str = \"test\") -> None:\n",
    "        with open(fileout, 'w') as f:\n",
    "            f.write(\"id,class\\n\")\n",
    "            if type_ == \"test\":\n",
    "                for data in self.testing_data.data:\n",
    "                    f.write(f\"{data.id},{self.classify_abstract(data.abstract)}\\n\")\n",
    "            elif type_ == \"train\":\n",
    "                for data in self.training_data.data:\n",
    "                    f.write(f\"{data.id},{self.classify_abstract(data.abstract)}\\n\")\n",
    "            else:\n",
    "                raise ValueError(\"Invalid type_ argument. Must be 'test', 'validation', or 'train'.\")\n",
    "\n",
    "\n",
    "    def get_word_probability(self, word: str, class_index: int) -> float:\n",
    "        '''\n",
    "        p(class|word) = p(word|class) * p(class) / p(word)\n",
    "        '''\n",
    "        word_count = self.word_counts[class_index].get(word, 0) + 1 # Add-One Laplace Smoothing\n",
    "        class_count = self.class_counts[class_index]\n",
    "        word_in_class = word_count / class_count\n",
    "        class_probability = self.class_probabilities[class_index]\n",
    "        word_in_data = sum([self.word_counts[i].get(word, 0) for i in range(len(self.classes))]) / len(self.training_data.data) + 1\n",
    "        return word_in_class * class_probability / word_in_data\n",
    "\n",
    "\n",
    "    def test_classify_abstract(self, abstract: str) -> str:\n",
    "        abstract_words = abstract.split()\n",
    "        class_probabilities = []\n",
    "        for i in range(len(self.classes)):\n",
    "            cur_class_probability = self.class_probabilities[i]\n",
    "            cur_class_probability = 1\n",
    "            for word in abstract_words:\n",
    "                cur_word_probability = self.get_word_probability(word, i)\n",
    "                if cur_word_probability == 0:\n",
    "                    continue\n",
    "                cur_class_probability *= cur_word_probability\n",
    "                print(f\"WORD: {word} | CLASS: {self.classes[i]} | PROB: {cur_class_probability}\")\n",
    "            class_probabilities.append(cur_class_probability)\n",
    "        max_class = self.classes[class_probabilities.index(max(class_probabilities))]\n",
    "        print(f\"Final Probabilities: {class_probabilities}\")\n",
    "        return max_class\n",
    "\n",
    "\n",
    "    def classify_abstract(self, abstract: str) -> str:\n",
    "        '''Classifies the abstract into one of the classes. Returns the class. Uses the Naive Bayesian Classifier algorithm.'''\n",
    "        abstract_words = abstract.split()\n",
    "        class_probabilities = []\n",
    "        for i in range(len(self.classes)):\n",
    "            cur_class_probability = self.class_probabilities[i]\n",
    "            cur_class_probability = 1\n",
    "            for word in abstract_words:\n",
    "                cur_word_probability = self.get_word_probability(word, i)\n",
    "                if cur_word_probability == 0:\n",
    "                    continue\n",
    "                cur_class_probability *= cur_word_probability\n",
    "            class_probabilities.append(cur_class_probability)\n",
    "        max_class = self.classes[class_probabilities.index(max(class_probabilities))]\n",
    "        return max_class\n",
    "\n",
    "\n",
    "    def get_class_counts(self) -> list[int]:\n",
    "        '''Returns the count of each class in the training data. Match the order of the classes with the order of self.classes (classes[i] -> class_counts[i])'''\n",
    "        class_counts = [0] * len(self.classes)\n",
    "        for data in self.training_data.data:\n",
    "            class_counts[self.classes.index(data.class_)] += 1\n",
    "        return class_counts\n",
    "    \n",
    "\n",
    "    def get_word_counts(self) -> list[dict[str, int]]:\n",
    "        '''Returns the count of each word in each class. Match the order of the classes with the order of self.classes (classes[i] -> word_counts[i])'''\n",
    "        word_counts = [{} for _ in range(len(self.classes))]\n",
    "        for data in self.training_data.data:\n",
    "            for word in data.abstract.split():\n",
    "                if word not in word_counts[self.classes.index(data.class_)]:\n",
    "                    word_counts[self.classes.index(data.class_)][word] = 1\n",
    "                else:\n",
    "                    word_counts[self.classes.index(data.class_)][word] += 1\n",
    "        return word_counts\n",
    "    \n",
    "\n",
    "    def save(self):\n",
    "        '''Saves the word counts to a txt file'''\n",
    "        with open(\"word_counts.txt\", 'w') as f:\n",
    "            for i in range(len(self.classes)):\n",
    "                f.write('-'*100 + '\\n')\n",
    "                f.write(f\"Class: {self.classes[i]}\\n\")\n",
    "                f.write(f\"Class Count: {self.class_counts[i]}\\n\")\n",
    "                f.write(f\"Class Probability: {self.class_probabilities[i]}\\n\\n\")\n",
    "                for word, count in self.word_counts[i].items():\n",
    "                    f.write(f\"{word}: {count}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: 2800\n",
      "Size of validation data: 1200\n",
      "Size of testing data: 1000\n",
      "Vocab size of training data: 25911\n",
      "Accuracy:  0.7891666666666667\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    classifier = ImprovedNaiveBayes(\n",
    "        validation_data_split=0.5,\n",
    "        alpha = 0.1\n",
    "    )\n",
    "    # classifier.run_test_data(\"new_output.csv\", type_=\"test\")\n",
    "    print(\"Accuracy: \", classifier.get_validation_accuracy())\n",
    "    classifier.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
