{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, id: int, class_: str | None, abstract: str) -> None:\n",
    "        self.id: int = id\n",
    "        self.class_: str = class_\n",
    "        self.abstract: str = abstract\n",
    "\n",
    "    def get_type(self) -> str:\n",
    "        '''Returns: \"test\" | \"train\"'''\n",
    "        return \"train\" if self.class_ else \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModel Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModel:\n",
    "    def __init__(self, data_path: str, do_shuffle: bool = False) -> None:\n",
    "        self.data: list[Data] = []\n",
    "        self.vocabulary_size: int = 0\n",
    "        if not os.path.exists(data_path):\n",
    "            self.data = []\n",
    "        else:\n",
    "            with open(data_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            if do_shuffle:\n",
    "                random.shuffle(lines) # shouldn't be used for the test data just in case Kaggle expects the data in order\n",
    "            for line in lines[1:]:\n",
    "                line_split = line.strip().replace('\"', '').split(',')\n",
    "                if len(line_split) == 3:\n",
    "                    id, class_, abstract = line_split\n",
    "                    data = Data(id, class_, abstract)\n",
    "                    self.data.append(data)\n",
    "                else:\n",
    "                    id, abstract = line_split\n",
    "                    data = Data(id, None, abstract)\n",
    "                    self.data.append(data)\n",
    "        self.vocabulary_size = self.get_vocabulary_size()\n",
    "\n",
    "\n",
    "    def set_data(self, data: list[Data]) -> None:\n",
    "        self.data = data\n",
    "        self.vocabulary_size = self.get_vocabulary_size()\n",
    "\n",
    "\n",
    "    def get_vocabulary_size(self) -> int:\n",
    "        '''Returns the number of unique words in the dataset.'''\n",
    "        vocabulary = set()\n",
    "        for data in self.data:\n",
    "            words = data.abstract.split()\n",
    "            for word in words:\n",
    "                vocabulary.add(word)\n",
    "        self.vocabulary_size = len(vocabulary)\n",
    "        return self.vocabulary_size\n",
    "\n",
    "\n",
    "    def split_model(self, proportion: float) -> 'DataModel':\n",
    "        '''Splits this dataset into two data sets based on the proportion of the current data to be in the new split.'''\n",
    "        split_data = DataModel('')\n",
    "        split_index = int(len(self.data) * proportion)\n",
    "        split_data.set_data(self.data[:split_index])\n",
    "        self.set_data(self.data[split_index:])\n",
    "        return split_data\n",
    "    \n",
    "    def eliminate_stop_words(self, stop_word_proportion) -> None:\n",
    "        '''\n",
    "        Stopwords include:\n",
    "        - Any word that is a single character\n",
    "        - Any word that is a number\n",
    "        - The top 10% of words that appear in all classes\n",
    "        '''\n",
    "        # first, get the single char / number words out\n",
    "        word_count = {}\n",
    "        num_eliminated = 0\n",
    "        for data in self.data:\n",
    "            words = data.abstract.split()\n",
    "            new_data = []\n",
    "            for word in words:\n",
    "                if len(word) > 1 and not word.isdigit():\n",
    "                    if word in word_count:\n",
    "                        word_count[word] += 1\n",
    "                    else:\n",
    "                        word_count[word] = 1\n",
    "                    new_data.append(word)\n",
    "                else:\n",
    "                    num_eliminated += 1\n",
    "            data.abstract = ' '.join(new_data)\n",
    "        # now eliminate the top (stop_word_proportion)% of words\n",
    "        stop_words = set()\n",
    "        num_words_to_eliminate = int(len(word_count) * stop_word_proportion)\n",
    "        for _ in range(num_words_to_eliminate):\n",
    "            max_word = max(word_count, key=word_count.get)\n",
    "            stop_words.add(max_word)\n",
    "            del word_count[max_word]\n",
    "            num_eliminated += 1\n",
    "        for data in self.data:\n",
    "            words = data.abstract.split()\n",
    "            new_data = []\n",
    "            for word in words:\n",
    "                if word not in stop_words:\n",
    "                    new_data.append(word)\n",
    "            data.abstract = ' '.join(new_data)\n",
    "        self.vocabulary_size = self.get_vocabulary_size()\n",
    "        print(f'Eliminated {num_eliminated} words. Including the {len(stop_words)} most common and {num_eliminated - len(stop_words)} single char / number words.')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardNaiveBayes:\n",
    "    def __init__(self) -> None:\n",
    "        self.training_data: DataModel = DataModel(\n",
    "            data_path=os.path.join(os.path.join(\"data\", \"trg.csv\")),\n",
    "            do_shuffle=False\n",
    "        )\n",
    "        self.testing_data: DataModel = DataModel(\n",
    "            data_path=os.path.join(os.path.join(\"data\", \"tst.csv\")),\n",
    "            do_shuffle=False\n",
    "        )\n",
    "        # self.training_data.eliminate_stop_words(0.02)\n",
    "        self.classes = [c for c in set([data.class_ for data in self.training_data.data])]\n",
    "        self.class_counts = self.get_class_counts()\n",
    "        self.class_probabilities = [count / len(self.training_data.data) for count in self.class_counts]\n",
    "        self.word_counts = self.get_word_counts()\n",
    "\n",
    "\n",
    "    def run_test_data(self, fileout: str, type_: str = \"test\") -> None:\n",
    "        with open(fileout, 'w') as f:\n",
    "            f.write(\"id,class\\n\")\n",
    "            if type_ == \"test\":\n",
    "                for data in self.testing_data.data:\n",
    "                    f.write(f\"{data.id},{self.classify_abstract(data.abstract)}\\n\")\n",
    "            elif type_ == \"train\":\n",
    "                for data in self.training_data.data:\n",
    "                    f.write(f\"{data.id},{self.classify_abstract(data.abstract)}\\n\")\n",
    "            else:\n",
    "                raise ValueError(\"Invalid type_ argument. Must be 'test', 'validation', or 'train'.\")\n",
    "\n",
    "\n",
    "    def get_word_probability(self, word: str, class_index: int) -> float:\n",
    "        '''\n",
    "        p(class|word) = p(word|class) * p(class) / p(word)\n",
    "        '''\n",
    "        word_count = self.word_counts[class_index].get(word, 0)\n",
    "        class_count = self.class_counts[class_index]\n",
    "        word_in_class = word_count / class_count\n",
    "        class_probability = self.class_probabilities[class_index]\n",
    "        word_in_data = sum([self.word_counts[i].get(word, 0) for i in range(len(self.classes))]) / len(self.training_data.data)\n",
    "        if word_in_data == 0:\n",
    "            return 0\n",
    "        return word_in_class * class_probability / word_in_data\n",
    "\n",
    "\n",
    "    def classify_abstract(self, abstract: str) -> str:\n",
    "        '''Classifies the abstract into one of the classes. Returns the class. Uses the Naive Bayesian Classifier algorithm.'''\n",
    "        abstract_words = abstract.split()\n",
    "        class_probabilities = []\n",
    "        for i in range(len(self.classes)):\n",
    "            cur_class_probability = self.class_probabilities[i]\n",
    "            cur_class_probability = 1\n",
    "            for word in abstract_words:\n",
    "                cur_word_probability = self.get_word_probability(word, i)\n",
    "                if cur_word_probability == 0:\n",
    "                    continue\n",
    "                cur_class_probability *= cur_word_probability\n",
    "            class_probabilities.append(cur_class_probability)\n",
    "        max_class = self.classes[class_probabilities.index(max(class_probabilities))]\n",
    "        return max_class\n",
    "\n",
    "\n",
    "    def get_class_counts(self) -> list[int]:\n",
    "        '''Returns the count of each class in the training data. Match the order of the classes with the order of self.classes (classes[i] -> class_counts[i])'''\n",
    "        class_counts = [0] * len(self.classes)\n",
    "        for data in self.training_data.data:\n",
    "            class_counts[self.classes.index(data.class_)] += 1\n",
    "        return class_counts\n",
    "    \n",
    "\n",
    "    def get_word_counts(self) -> list[dict[str, int]]:\n",
    "        '''Returns the count of each word in each class. Match the order of the classes with the order of self.classes (classes[i] -> word_counts[i])'''\n",
    "        word_counts = [{} for _ in range(len(self.classes))]\n",
    "        for data in self.training_data.data:\n",
    "            for word in data.abstract.split():\n",
    "                if word not in word_counts[self.classes.index(data.class_)]:\n",
    "                    word_counts[self.classes.index(data.class_)][word] = 1\n",
    "                else:\n",
    "                    word_counts[self.classes.index(data.class_)][word] += 1\n",
    "        return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminated 41175 words. Including the 602 most common and 40573 single char / number words.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    classifier = StandardNaiveBayes()\n",
    "    classifier.run_test_data(\"standard_output.csv\", type_=\"test\")\n",
    "    '''\n",
    "    -->    Accuracy Obtained: 0.800 (Kaggle)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of Standard Naive Bayesian Classifier\n",
    "...\n",
    "\n",
    "# Improvements \n",
    "...\n",
    "- Add-1 LaPlace Smoothing\n",
    "- Multinomial Naive Bayes\n",
    "- Vocab Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MN_DataModel:\n",
    "    def __init__(self, data_path: str, do_shuffle: bool = False) -> None:\n",
    "        '''\n",
    "        Difference between MN_DataModel and DataModel is that it will combine multiple words as features.\n",
    "        '''\n",
    "        self.data: list[Data] = []\n",
    "        self.vocabulary_size: int = 0\n",
    "        if not os.path.exists(data_path):\n",
    "            self.data = []\n",
    "        else:\n",
    "            with open(data_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            if do_shuffle:\n",
    "                random.shuffle(lines) # shouldn't be used for the test data just in case Kaggle expects the data in order\n",
    "            for line in lines[1:]:\n",
    "                line_split = line.strip().replace('\"', '').split(',')\n",
    "                if len(line_split) == 3:\n",
    "                    id, class_, abstract = line_split\n",
    "                    data = Data(id, class_, abstract)\n",
    "                    self.data.append(data)\n",
    "                else:\n",
    "                    id, abstract = line_split\n",
    "                    data = Data(id, None, abstract)\n",
    "                    self.data.append(data)\n",
    "        self.vocabulary_size = self.get_vocabulary_size()\n",
    "\n",
    "\n",
    "    def set_data(self, data: list[Data]) -> None:\n",
    "        self.data = data\n",
    "        self.vocabulary_size = self.get_vocabulary_size()\n",
    "\n",
    "\n",
    "    def get_vocabulary_size(self) -> int:\n",
    "        '''Returns the number of unique words in the dataset.'''\n",
    "        vocabulary = set()\n",
    "        for data in self.data:\n",
    "            # words = data.abstract.split()\n",
    "            # coupling words\n",
    "            words = self.split_abstract(data.abstract)\n",
    "            for word in words:\n",
    "                vocabulary.add(word)\n",
    "        self.vocabulary_size = len(vocabulary)\n",
    "        return self.vocabulary_size\n",
    "\n",
    "\n",
    "    def split_model(self, proportion: float) -> 'DataModel':\n",
    "        '''Splits this dataset into two data sets based on the proportion of the current data to be in the new split.'''\n",
    "        split_data = DataModel('')\n",
    "        split_index = int(len(self.data) * proportion)\n",
    "        split_data.set_data(self.data[:split_index])\n",
    "        self.set_data(self.data[split_index:])\n",
    "        return split_data\n",
    "    \n",
    "\n",
    "    def split_abstract(self, abstract: str) -> list[str]:\n",
    "        '''Splits the abstract into a list of paired words.'''\n",
    "        words = abstract.split()\n",
    "        combined_words = []\n",
    "        for i in range(0, len(words), 2):\n",
    "            if i + 1 < len(words):\n",
    "                combined_words.append(words[i] + ' ' + words[i + 1])\n",
    "            else:\n",
    "                combined_words.append(words[i])\n",
    "        return combined_words\n",
    "\n",
    "    def eliminate_stop_words(self, stop_word_proportion) -> None:\n",
    "        '''\n",
    "        Stopwords include:\n",
    "        - Any word that is a single character\n",
    "        - Any word that is a number\n",
    "        - The top 10% of words that appear in all classes\n",
    "        '''\n",
    "        # first, get the single char / number words out\n",
    "        word_count = {}\n",
    "        num_eliminated = 0\n",
    "        for data in self.data:\n",
    "            words = self.split_abstract(data.abstract)\n",
    "            new_data = []\n",
    "            for word in words:\n",
    "                if len(word) > 1 and not word.isdigit():\n",
    "                    if word in word_count:\n",
    "                        word_count[word] += 1\n",
    "                    else:\n",
    "                        word_count[word] = 1\n",
    "                    new_data.append(word)\n",
    "                else:\n",
    "                    num_eliminated += 1\n",
    "            data.abstract = ' '.join(new_data)\n",
    "        # now eliminate the top (stop_word_proportion)% of words\n",
    "        stop_words = set()\n",
    "        num_words_to_eliminate = int(len(word_count) * stop_word_proportion)\n",
    "        for _ in range(num_words_to_eliminate):\n",
    "            max_word = max(word_count, key=word_count.get)\n",
    "            stop_words.add(max_word)\n",
    "            del word_count[max_word]\n",
    "            num_eliminated += 1\n",
    "        for data in self.data:\n",
    "            words = self.split_abstract(data.abstract)\n",
    "            new_data = []\n",
    "            for word in words:\n",
    "                if word not in stop_words:\n",
    "                    new_data.append(word)\n",
    "            data.abstract = ' '.join(new_data)\n",
    "        self.vocabulary_size = self.get_vocabulary_size()\n",
    "        print(f'Eliminated {num_eliminated} words. Including the {len(stop_words)} most common and {num_eliminated - len(stop_words)} single char / number words.')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedNaiveBayes:\n",
    "    def __init__(self, validation_data_split: float, alpha: float, stop_word_proportion: float, auto_load_data: bool = True) -> None:\n",
    "        if auto_load_data:\n",
    "            self.training_data: DataModel = DataModel(\n",
    "                data_path=os.path.join(os.path.join(\"data\", \"trg.csv\")),\n",
    "                do_shuffle=True\n",
    "            )\n",
    "            self.testing_data: DataModel = DataModel(\n",
    "                data_path=os.path.join(os.path.join(\"data\", \"tst.csv\")),\n",
    "                do_shuffle=False\n",
    "            )\n",
    "            self.training_data.eliminate_stop_words(stop_word_proportion)\n",
    "            if validation_data_split > 0.0:\n",
    "                self.validation_data: DataModel = self.training_data.split_model(validation_data_split)\n",
    "            else:\n",
    "                self.validation_data = None\n",
    "            self.alpha = alpha\n",
    "            self.vocab_size = self.training_data.vocabulary_size\n",
    "            self.classes = [c for c in set([data.class_ for data in self.training_data.data])]\n",
    "            self.class_counts = self.get_class_counts()\n",
    "            self.class_probabilities = [count / len(self.training_data.data) for count in self.class_counts]\n",
    "            self.word_counts = self.get_word_counts()\n",
    "        else:\n",
    "            self.training_data = DataModel('')\n",
    "            self.testing_data = DataModel('')\n",
    "            self.validation_data = None\n",
    "            self.alpha = alpha\n",
    "            self.vocab_size = 0\n",
    "            self.classes = []\n",
    "            self.class_counts = []\n",
    "            self.class_probabilities = []\n",
    "            self.word_counts = []\n",
    "\n",
    "\n",
    "    def set_data(self, training_data: DataModel = None, testing_data: DataModel = None, validation_data: DataModel = None) -> None:\n",
    "        if training_data:\n",
    "            self.training_data.set_data(training_data.data)\n",
    "        if testing_data:\n",
    "            self.testing_data.set_data(testing_data.data)\n",
    "        if validation_data:\n",
    "            if self.validation_data:\n",
    "                self.validation_data.set_data(validation_data.data)\n",
    "            else:\n",
    "                self.validation_data = DataModel('')\n",
    "                self.validation_data.set_data(validation_data.data)\n",
    "        self.vocab_size = self.training_data.vocabulary_size\n",
    "        self.classes = [c for c in set([data.class_ for data in self.training_data.data])]\n",
    "        if self.classes == []:\n",
    "            print(f\"Classes: {self.classes}\")\n",
    "            print(f\"{self.training_data.data[0].class_}, {self.training_data.data[0].abstract}\")\n",
    "            print(f\"{self.training_data.data[1].class_}, {self.training_data.data[1].abstract}\")\n",
    "        self.class_counts = self.get_class_counts()\n",
    "        self.class_probabilities = [count / len(self.training_data.data) for count in self.class_counts]\n",
    "        self.word_counts = self.get_word_counts()\n",
    "\n",
    "\n",
    "    def get_validation_accuracy(self) -> float:\n",
    "        if not self.validation_data:\n",
    "            print(f\"Model has no validation set.\")\n",
    "            return 0\n",
    "        correct = 0\n",
    "        for data in self.validation_data.data:\n",
    "            predicted_class = self.classify_abstract(data.abstract)\n",
    "            # print(f\"Predicted: {predicted_class} | Actual: {data.class_}\")\n",
    "            if predicted_class == data.class_:\n",
    "                correct += 1\n",
    "        return correct / len(self.validation_data.data)\n",
    "\n",
    "\n",
    "    def run_test_data(self, fileout: str, type_: str = \"test\") -> None:\n",
    "        with open(fileout, 'w') as f:\n",
    "            f.write(\"id,class\\n\")\n",
    "            if type_ == \"test\":\n",
    "                for data in self.testing_data.data:\n",
    "                    f.write(f\"{data.id},{self.classify_abstract(data.abstract)}\\n\")\n",
    "            elif type_ == \"train\":\n",
    "                for data in self.training_data.data:\n",
    "                    f.write(f\"{data.id},{self.classify_abstract(data.abstract)}\\n\")\n",
    "            else:\n",
    "                raise ValueError(\"Invalid type_ argument. Must be 'test', 'validation', or 'train'.\")\n",
    "    \n",
    "    \n",
    "    def get_word_probability(self, word: str, class_index: int) -> float:\n",
    "        '''\n",
    "        p(class|word) = p(word|class) * p(class) / p(word) -- NEW (Dirichlet)\n",
    "        '''\n",
    "        word_count = self.word_counts[class_index].get(word, 0)\n",
    "        class_count = self.class_counts[class_index]\n",
    "        word_in_class = (word_count + self.alpha) / (class_count + self.alpha * self.vocab_size)\n",
    "        class_probability = self.class_probabilities[class_index]\n",
    "        word_in_data = (sum([self.word_counts[i].get(word, 0) for i in range(len(self.classes))]) + self.alpha) / (len(self.training_data.data) + self.alpha * self.vocab_size)\n",
    "        return word_in_class * class_probability / word_in_data\n",
    "\n",
    "\n",
    "    def classify_abstract(self, abstract: str) -> str:\n",
    "        '''Classifies the abstract into one of the classes. Returns the class. Uses the Naive Bayesian Classifier algorithm.'''\n",
    "        abstract_words = abstract.split()\n",
    "\n",
    "        # testing with mn words\n",
    "        # abstract_words = self.training_data.split_abstract(abstract)\n",
    "        abstract_words = abstract.split()\n",
    "\n",
    "        class_probabilities = []\n",
    "        for i in range(len(self.classes)):\n",
    "            cur_class_probability = self.class_probabilities[i]\n",
    "            cur_class_probability = 1\n",
    "            for word in abstract_words:\n",
    "                cur_word_probability = self.get_word_probability(word, i)\n",
    "                if cur_word_probability == 0:\n",
    "                    continue\n",
    "                cur_class_probability *= cur_word_probability\n",
    "            class_probabilities.append(cur_class_probability)\n",
    "        max_class = self.classes[class_probabilities.index(max(class_probabilities))]\n",
    "        return max_class\n",
    "    \n",
    "\n",
    "\n",
    "    def get_class_counts(self) -> list[int]:\n",
    "        '''Returns the count of each class in the training data. Match the order of the classes with the order of self.classes (classes[i] -> class_counts[i])'''\n",
    "        class_counts = [0] * len(self.classes)\n",
    "        for data in self.training_data.data:\n",
    "            class_counts[self.classes.index(data.class_)] += 1\n",
    "        return class_counts\n",
    "    \n",
    "\n",
    "    def get_word_counts(self) -> list[dict[str, int]]:\n",
    "        '''Returns the count of each word in each class. Match the order of the classes with the order of self.classes (classes[i] -> word_counts[i])'''\n",
    "        word_counts = [{} for _ in range(len(self.classes))]\n",
    "        for data in self.training_data.data:\n",
    "            # testing with mn words\n",
    "            # words = self.training_data.split_abstract(data.abstract)\n",
    "            words = data.abstract.split()\n",
    "            for word in words:\n",
    "                if word not in word_counts[self.classes.index(data.class_)]:\n",
    "                    word_counts[self.classes.index(data.class_)][word] = 1\n",
    "                else:\n",
    "                    word_counts[self.classes.index(data.class_)][word] += 1\n",
    "        return word_counts\n",
    "    \n",
    "\n",
    "    def save(self):\n",
    "        '''Saves the word counts to a txt file'''\n",
    "        with open(\"word_counts.txt\", 'w') as f:\n",
    "            for i in range(len(self.classes)):\n",
    "                f.write('-'*100 + '\\n')\n",
    "                f.write(f\"Class: {self.classes[i]}\\n\")\n",
    "                f.write(f\"Class Count: {self.class_counts[i]}\\n\")\n",
    "                f.write(f\"Class Probability: {self.class_probabilities[i]}\\n\\n\")\n",
    "                for word, count in self.word_counts[i].items():\n",
    "                    f.write(f\"{word}: {count}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminated 2345 words. Including the 2309 most common and 36 single char / number words.\n",
      "Eliminated 2345 words. Including the 2309 most common and 36 single char / number words.\n",
      "1: {'E': 4, 'B': 1}\n",
      "2: {'E': 5}\n",
      "3: {'E': 5}\n",
      "4: {'E': 5}\n",
      "5: {'E': 5}\n",
      "6: {'E': 5}\n",
      "7: {'E': 5}\n",
      "8: {'E': 1, 'B': 4}\n",
      "9: {'E': 5}\n",
      "10: {'E': 5}\n",
      "11: {'B': 2, 'E': 3}\n",
      "12: {'E': 5}\n",
      "13: {'B': 5}\n",
      "14: {'B': 5}\n",
      "15: {'E': 5}\n",
      "16: {'E': 5}\n",
      "17: {'E': 5}\n",
      "18: {'E': 4, 'B': 1}\n",
      "19: {'E': 5}\n",
      "20: {'E': 5}\n",
      "21: {'B': 4, 'E': 1}\n",
      "22: {'E': 5}\n",
      "23: {'B': 5}\n",
      "24: {'B': 4, 'E': 1}\n",
      "25: {'E': 5}\n",
      "26: {'E': 4, 'B': 1}\n",
      "27: {'E': 5}\n",
      "28: {'E': 3, 'B': 2}\n",
      "29: {'B': 4, 'E': 1}\n",
      "30: {'E': 5}\n",
      "31: {'E': 5}\n",
      "32: {'B': 5}\n",
      "33: {'E': 4, 'B': 1}\n",
      "34: {'E': 5}\n",
      "35: {'B': 4, 'E': 1}\n",
      "36: {'E': 5}\n",
      "37: {'E': 5}\n",
      "38: {'E': 5}\n",
      "39: {'B': 5}\n",
      "40: {'E': 5}\n",
      "41: {'E': 5}\n",
      "42: {'E': 5}\n",
      "43: {'E': 4, 'B': 1}\n",
      "44: {'E': 3, 'B': 2}\n",
      "45: {'E': 5}\n",
      "46: {'E': 4, 'B': 1}\n",
      "47: {'E': 5}\n",
      "48: {'B': 5}\n",
      "49: {'B': 5}\n",
      "50: {'E': 5}\n",
      "51: {'E': 4, 'B': 1}\n",
      "52: {'E': 5}\n",
      "53: {'E': 5}\n",
      "54: {'B': 5}\n",
      "55: {'E': 5}\n",
      "56: {'E': 4, 'B': 1}\n",
      "57: {'B': 1, 'E': 4}\n",
      "58: {'B': 1, 'E': 4}\n",
      "59: {'B': 5}\n",
      "60: {'E': 5}\n",
      "61: {'E': 5}\n",
      "62: {'E': 5}\n",
      "63: {'E': 5}\n",
      "64: {'B': 4, 'E': 1}\n",
      "65: {'E': 5}\n",
      "66: {'B': 4, 'E': 1}\n",
      "67: {'E': 5}\n",
      "68: {'E': 5}\n",
      "69: {'E': 5}\n",
      "70: {'E': 5}\n",
      "71: {'E': 5}\n",
      "72: {'E': 5}\n",
      "73: {'E': 5}\n",
      "74: {'B': 3, 'E': 2}\n",
      "75: {'E': 5}\n",
      "76: {'E': 5}\n",
      "77: {'E': 5}\n",
      "78: {'E': 5}\n",
      "79: {'E': 4, 'B': 1}\n",
      "80: {'E': 3, 'B': 2}\n",
      "81: {'E': 3, 'B': 1, 'V': 1}\n",
      "Moderator Intervention: E\n",
      "82: {'E': 4, 'B': 1}\n",
      "83: {'E': 5}\n",
      "84: {'E': 5}\n",
      "85: {'E': 5}\n",
      "86: {'E': 5}\n",
      "87: {'E': 5}\n",
      "88: {'E': 5}\n",
      "89: {'B': 4, 'E': 1}\n",
      "90: {'E': 5}\n",
      "91: {'E': 5}\n",
      "92: {'E': 4, 'B': 1}\n",
      "93: {'B': 5}\n",
      "94: {'B': 5}\n",
      "95: {'E': 5}\n",
      "96: {'E': 5}\n",
      "97: {'E': 3, 'B': 2}\n",
      "98: {'E': 5}\n",
      "99: {'E': 4, 'B': 1}\n",
      "100: {'B': 5}\n",
      "101: {'E': 3, 'B': 2}\n",
      "102: {'B': 3, 'E': 2}\n",
      "103: {'E': 4, 'B': 1}\n",
      "104: {'E': 5}\n",
      "105: {'E': 5}\n",
      "106: {'E': 4, 'B': 1}\n",
      "107: {'E': 4, 'B': 1}\n",
      "108: {'E': 5}\n",
      "109: {'E': 2, 'B': 3}\n",
      "110: {'B': 5}\n",
      "111: {'B': 5}\n",
      "112: {'E': 5}\n",
      "113: {'E': 5}\n",
      "114: {'E': 3, 'B': 2}\n",
      "115: {'B': 4, 'E': 1}\n",
      "116: {'B': 1, 'E': 4}\n",
      "117: {'B': 3, 'E': 2}\n",
      "118: {'B': 5}\n",
      "119: {'E': 4, 'B': 1}\n",
      "120: {'E': 5}\n",
      "121: {'E': 5}\n",
      "122: {'E': 5}\n",
      "123: {'E': 5}\n",
      "124: {'E': 5}\n",
      "125: {'E': 4, 'B': 1}\n",
      "126: {'E': 3, 'B': 2}\n",
      "127: {'E': 4, 'B': 1}\n",
      "128: {'E': 5}\n",
      "129: {'E': 4, 'B': 1}\n",
      "130: {'B': 5}\n",
      "131: {'E': 5}\n",
      "132: {'E': 5}\n",
      "133: {'E': 3, 'B': 2}\n",
      "134: {'E': 5}\n",
      "135: {'E': 5}\n",
      "136: {'B': 4, 'E': 1}\n",
      "137: {'B': 4, 'E': 1}\n",
      "138: {'E': 5}\n",
      "139: {'E': 5}\n",
      "140: {'B': 4, 'E': 1}\n",
      "141: {'E': 5}\n",
      "142: {'B': 5}\n",
      "143: {'E': 5}\n",
      "144: {'E': 5}\n",
      "145: {'B': 4, 'E': 1}\n",
      "146: {'E': 5}\n",
      "147: {'E': 5}\n",
      "148: {'B': 4, 'E': 1}\n",
      "149: {'B': 5}\n",
      "150: {'E': 5}\n",
      "151: {'E': 5}\n",
      "152: {'B': 5}\n",
      "153: {'E': 5}\n",
      "154: {'E': 5}\n",
      "155: {'E': 5}\n",
      "156: {'B': 4, 'E': 1}\n",
      "157: {'B': 5}\n",
      "158: {'A': 4, 'B': 1}\n",
      "159: {'E': 4, 'B': 1}\n",
      "160: {'E': 5}\n",
      "161: {'E': 5}\n",
      "162: {'E': 5}\n",
      "163: {'B': 5}\n",
      "164: {'B': 5}\n",
      "165: {'B': 1, 'V': 1, 'E': 3}\n",
      "Moderator Intervention: E\n",
      "166: {'E': 5}\n",
      "167: {'E': 4, 'B': 1}\n",
      "168: {'E': 5}\n",
      "169: {'E': 5}\n",
      "170: {'E': 5}\n",
      "171: {'E': 5}\n",
      "172: {'E': 3, 'B': 2}\n",
      "173: {'B': 4, 'E': 1}\n",
      "174: {'E': 5}\n",
      "175: {'B': 5}\n",
      "176: {'E': 5}\n",
      "177: {'B': 4, 'E': 1}\n",
      "178: {'E': 5}\n",
      "179: {'E': 5}\n",
      "180: {'E': 5}\n",
      "181: {'E': 5}\n",
      "182: {'E': 4, 'B': 1}\n",
      "183: {'E': 5}\n",
      "184: {'E': 5}\n",
      "185: {'B': 4, 'E': 1}\n",
      "186: {'E': 4, 'B': 1}\n",
      "187: {'E': 5}\n",
      "188: {'E': 3, 'B': 2}\n",
      "189: {'B': 5}\n",
      "190: {'E': 5}\n",
      "191: {'A': 5}\n",
      "192: {'B': 5}\n",
      "193: {'B': 5}\n",
      "194: {'E': 5}\n",
      "195: {'B': 5}\n",
      "196: {'A': 5}\n",
      "197: {'E': 5}\n",
      "198: {'E': 5}\n",
      "199: {'B': 2, 'E': 3}\n",
      "200: {'E': 5}\n",
      "201: {'E': 5}\n",
      "202: {'E': 4, 'B': 1}\n",
      "203: {'E': 5}\n",
      "204: {'E': 5}\n",
      "205: {'E': 5}\n",
      "206: {'E': 1, 'B': 4}\n",
      "207: {'E': 5}\n",
      "208: {'B': 4, 'E': 1}\n",
      "209: {'E': 5}\n",
      "210: {'E': 3, 'A': 1, 'B': 1}\n",
      "Moderator Intervention: A\n",
      "211: {'B': 5}\n",
      "212: {'E': 2, 'B': 3}\n",
      "213: {'E': 5}\n",
      "214: {'E': 5}\n",
      "215: {'E': 5}\n",
      "216: {'E': 5}\n",
      "217: {'E': 4, 'B': 1}\n",
      "218: {'E': 5}\n",
      "219: {'E': 5}\n",
      "220: {'E': 5}\n",
      "221: {'E': 5}\n",
      "222: {'B': 5}\n",
      "223: {'E': 5}\n",
      "224: {'E': 5}\n",
      "225: {'B': 5}\n",
      "226: {'E': 5}\n",
      "227: {'B': 3, 'E': 2}\n",
      "228: {'E': 5}\n",
      "229: {'E': 5}\n",
      "230: {'B': 5}\n",
      "231: {'E': 4, 'V': 1}\n",
      "232: {'E': 5}\n",
      "233: {'E': 5}\n",
      "234: {'E': 5}\n",
      "235: {'B': 4, 'E': 1}\n",
      "236: {'E': 5}\n",
      "237: {'E': 5}\n",
      "238: {'E': 5}\n",
      "239: {'E': 5}\n",
      "240: {'E': 5}\n",
      "241: {'E': 4, 'B': 1}\n",
      "242: {'A': 2, 'E': 3}\n",
      "243: {'E': 5}\n",
      "244: {'E': 5}\n",
      "245: {'B': 1, 'E': 4}\n",
      "246: {'B': 5}\n",
      "247: {'E': 5}\n",
      "248: {'E': 4, 'B': 1}\n",
      "249: {'B': 3, 'E': 2}\n",
      "250: {'B': 4, 'E': 1}\n",
      "251: {'E': 5}\n",
      "252: {'E': 5}\n",
      "253: {'B': 5}\n",
      "254: {'E': 5}\n",
      "255: {'E': 5}\n",
      "256: {'B': 5}\n",
      "257: {'E': 5}\n",
      "258: {'E': 5}\n",
      "259: {'E': 5}\n",
      "260: {'E': 5}\n",
      "261: {'B': 4, 'E': 1}\n",
      "262: {'B': 4, 'E': 1}\n",
      "263: {'E': 5}\n",
      "264: {'E': 5}\n",
      "265: {'E': 4, 'B': 1}\n",
      "266: {'E': 4, 'B': 1}\n",
      "267: {'B': 4, 'E': 1}\n",
      "268: {'B': 5}\n",
      "269: {'E': 5}\n",
      "270: {'E': 5}\n",
      "271: {'E': 5}\n",
      "272: {'E': 4, 'B': 1}\n",
      "273: {'B': 5}\n",
      "274: {'E': 5}\n",
      "275: {'E': 5}\n",
      "276: {'E': 4, 'B': 1}\n",
      "277: {'E': 3, 'B': 2}\n",
      "278: {'E': 5}\n",
      "279: {'E': 5}\n",
      "280: {'E': 5}\n",
      "281: {'E': 5}\n",
      "282: {'E': 5}\n",
      "283: {'E': 5}\n",
      "284: {'E': 5}\n",
      "285: {'B': 3, 'E': 2}\n",
      "286: {'E': 5}\n",
      "287: {'E': 2, 'B': 3}\n",
      "288: {'E': 5}\n",
      "289: {'E': 5}\n",
      "290: {'B': 4, 'E': 1}\n",
      "291: {'B': 5}\n",
      "292: {'B': 5}\n",
      "293: {'E': 5}\n",
      "294: {'A': 4, 'B': 1}\n",
      "295: {'E': 5}\n",
      "296: {'B': 5}\n",
      "297: {'E': 5}\n",
      "298: {'E': 5}\n",
      "299: {'E': 5}\n",
      "300: {'E': 2, 'B': 3}\n",
      "301: {'E': 5}\n",
      "302: {'B': 4, 'E': 1}\n",
      "303: {'E': 5}\n",
      "304: {'E': 5}\n",
      "305: {'E': 5}\n",
      "306: {'A': 2, 'E': 3}\n",
      "307: {'B': 5}\n",
      "308: {'B': 5}\n",
      "309: {'E': 5}\n",
      "310: {'E': 5}\n",
      "311: {'E': 4, 'B': 1}\n",
      "312: {'E': 5}\n",
      "313: {'E': 1, 'A': 2, 'B': 2}\n",
      "Moderator Intervention: E\n",
      "314: {'E': 4, 'B': 1}\n",
      "315: {'B': 2, 'E': 3}\n",
      "316: {'E': 5}\n",
      "317: {'E': 5}\n",
      "318: {'E': 5}\n",
      "319: {'E': 5}\n",
      "320: {'B': 5}\n",
      "321: {'B': 5}\n",
      "322: {'B': 1, 'E': 4}\n",
      "323: {'E': 5}\n",
      "324: {'E': 5}\n",
      "325: {'E': 5}\n",
      "326: {'B': 5}\n",
      "327: {'E': 5}\n",
      "328: {'E': 4, 'B': 1}\n",
      "329: {'E': 1, 'B': 4}\n",
      "330: {'B': 4, 'E': 1}\n",
      "331: {'E': 5}\n",
      "332: {'B': 5}\n",
      "333: {'E': 5}\n",
      "334: {'B': 5}\n",
      "335: {'A': 5}\n",
      "336: {'E': 4, 'B': 1}\n",
      "337: {'E': 5}\n",
      "338: {'E': 5}\n",
      "339: {'E': 5}\n",
      "340: {'E': 5}\n",
      "341: {'E': 5}\n",
      "342: {'B': 3, 'E': 2}\n",
      "343: {'A': 1, 'E': 4}\n",
      "344: {'E': 5}\n",
      "345: {'E': 5}\n",
      "346: {'A': 2, 'B': 1, 'E': 2}\n",
      "Moderator Intervention: A\n",
      "347: {'B': 1, 'E': 4}\n",
      "348: {'B': 5}\n",
      "349: {'E': 5}\n",
      "350: {'E': 2, 'B': 3}\n",
      "351: {'E': 5}\n",
      "352: {'E': 4, 'B': 1}\n",
      "353: {'E': 5}\n",
      "354: {'E': 4, 'B': 1}\n",
      "355: {'E': 5}\n",
      "356: {'E': 5}\n",
      "357: {'E': 4, 'B': 1}\n",
      "358: {'B': 3, 'E': 2}\n",
      "359: {'E': 5}\n",
      "360: {'E': 5}\n",
      "361: {'B': 3, 'E': 2}\n",
      "362: {'E': 5}\n",
      "363: {'E': 5}\n",
      "364: {'E': 5}\n",
      "365: {'B': 5}\n",
      "366: {'B': 4, 'E': 1}\n",
      "367: {'E': 5}\n",
      "368: {'E': 5}\n",
      "369: {'E': 5}\n",
      "370: {'B': 5}\n",
      "371: {'E': 5}\n",
      "372: {'E': 5}\n",
      "373: {'E': 5}\n",
      "374: {'B': 5}\n",
      "375: {'E': 5}\n",
      "376: {'E': 4, 'B': 1}\n",
      "377: {'B': 1, 'E': 4}\n",
      "378: {'E': 5}\n",
      "379: {'E': 5}\n",
      "380: {'E': 2, 'B': 2, 'A': 1}\n",
      "Moderator Intervention: E\n",
      "381: {'B': 1, 'E': 4}\n",
      "382: {'E': 4, 'B': 1}\n",
      "383: {'E': 5}\n",
      "384: {'E': 5}\n",
      "385: {'E': 5}\n",
      "386: {'B': 1, 'E': 4}\n",
      "387: {'A': 5}\n",
      "388: {'B': 4, 'E': 1}\n",
      "389: {'E': 4, 'B': 1}\n",
      "390: {'E': 5}\n",
      "391: {'E': 5}\n",
      "392: {'B': 5}\n",
      "393: {'E': 4, 'B': 1}\n",
      "394: {'E': 4, 'B': 1}\n",
      "395: {'E': 2, 'B': 3}\n",
      "396: {'E': 5}\n",
      "397: {'E': 5}\n",
      "398: {'E': 5}\n",
      "399: {'E': 5}\n",
      "400: {'E': 5}\n",
      "401: {'E': 5}\n",
      "402: {'E': 5}\n",
      "403: {'E': 5}\n",
      "404: {'B': 5}\n",
      "405: {'E': 5}\n",
      "406: {'E': 5}\n",
      "407: {'B': 1, 'E': 4}\n",
      "408: {'E': 5}\n",
      "409: {'E': 5}\n",
      "410: {'E': 5}\n",
      "411: {'E': 5}\n",
      "412: {'B': 2, 'E': 3}\n",
      "413: {'E': 5}\n",
      "414: {'B': 4, 'E': 1}\n",
      "415: {'E': 1, 'B': 4}\n",
      "416: {'E': 5}\n",
      "417: {'E': 4, 'B': 1}\n",
      "418: {'E': 5}\n",
      "419: {'E': 5}\n",
      "420: {'B': 5}\n",
      "421: {'E': 5}\n",
      "422: {'E': 1, 'B': 4}\n",
      "423: {'E': 5}\n",
      "424: {'E': 5}\n",
      "425: {'E': 5}\n",
      "426: {'E': 5}\n",
      "427: {'E': 5}\n",
      "428: {'E': 5}\n",
      "429: {'E': 5}\n",
      "430: {'B': 5}\n",
      "431: {'E': 5}\n",
      "432: {'E': 5}\n",
      "433: {'E': 5}\n",
      "434: {'E': 5}\n",
      "435: {'E': 5}\n",
      "436: {'E': 5}\n",
      "437: {'E': 3, 'B': 2}\n",
      "438: {'E': 5}\n",
      "439: {'B': 5}\n",
      "440: {'B': 1, 'E': 4}\n",
      "441: {'E': 5}\n",
      "442: {'E': 5}\n",
      "443: {'E': 5}\n",
      "444: {'E': 5}\n",
      "445: {'E': 3, 'B': 2}\n",
      "446: {'B': 5}\n",
      "447: {'E': 5}\n",
      "448: {'A': 4, 'B': 1}\n",
      "449: {'E': 5}\n",
      "450: {'B': 5}\n",
      "451: {'B': 3, 'E': 2}\n",
      "452: {'E': 5}\n",
      "453: {'B': 4, 'E': 1}\n",
      "454: {'E': 5}\n",
      "455: {'E': 5}\n",
      "456: {'E': 5}\n",
      "457: {'E': 5}\n",
      "458: {'E': 5}\n",
      "459: {'E': 4, 'B': 1}\n",
      "460: {'E': 5}\n",
      "461: {'E': 5}\n",
      "462: {'E': 5}\n",
      "463: {'E': 4, 'V': 1}\n",
      "464: {'E': 5}\n",
      "465: {'E': 5}\n",
      "466: {'E': 5}\n",
      "467: {'B': 5}\n",
      "468: {'B': 4, 'E': 1}\n",
      "469: {'B': 2, 'E': 3}\n",
      "470: {'B': 5}\n",
      "471: {'E': 5}\n",
      "472: {'E': 5}\n",
      "473: {'B': 5}\n",
      "474: {'E': 5}\n",
      "475: {'E': 5}\n",
      "476: {'E': 5}\n",
      "477: {'B': 4, 'E': 1}\n",
      "478: {'E': 5}\n",
      "479: {'E': 5}\n",
      "480: {'E': 5}\n",
      "481: {'E': 4, 'B': 1}\n",
      "482: {'E': 5}\n",
      "483: {'B': 4, 'E': 1}\n",
      "484: {'E': 5}\n",
      "485: {'E': 5}\n",
      "486: {'E': 5}\n",
      "487: {'E': 5}\n",
      "488: {'E': 5}\n",
      "489: {'E': 5}\n",
      "490: {'B': 3, 'E': 2}\n",
      "491: {'E': 5}\n",
      "492: {'E': 5}\n",
      "493: {'E': 5}\n",
      "494: {'B': 3, 'E': 2}\n",
      "495: {'E': 5}\n",
      "496: {'E': 5}\n",
      "497: {'B': 2, 'E': 3}\n",
      "498: {'B': 5}\n",
      "499: {'E': 3, 'B': 2}\n",
      "500: {'B': 2, 'E': 3}\n",
      "501: {'E': 5}\n",
      "502: {'E': 5}\n",
      "503: {'B': 5}\n",
      "504: {'B': 5}\n",
      "505: {'E': 5}\n",
      "506: {'E': 5}\n",
      "507: {'E': 5}\n",
      "508: {'E': 5}\n",
      "509: {'E': 5}\n",
      "510: {'B': 5}\n",
      "511: {'E': 3, 'B': 2}\n",
      "512: {'E': 5}\n",
      "513: {'A': 5}\n",
      "514: {'E': 5}\n",
      "515: {'E': 5}\n",
      "516: {'E': 5}\n",
      "517: {'B': 5}\n",
      "518: {'B': 5}\n",
      "519: {'B': 5}\n",
      "520: {'E': 5}\n",
      "521: {'B': 5}\n",
      "522: {'B': 3, 'E': 2}\n",
      "523: {'E': 5}\n",
      "524: {'E': 5}\n",
      "525: {'E': 5}\n",
      "526: {'E': 1, 'B': 4}\n",
      "527: {'A': 5}\n",
      "528: {'B': 1, 'E': 4}\n",
      "529: {'B': 5}\n",
      "530: {'E': 5}\n",
      "531: {'B': 5}\n",
      "532: {'E': 5}\n",
      "533: {'E': 5}\n",
      "534: {'E': 5}\n",
      "535: {'B': 5}\n",
      "536: {'E': 5}\n",
      "537: {'B': 1, 'E': 4}\n",
      "538: {'A': 5}\n",
      "539: {'A': 2, 'E': 3}\n",
      "540: {'B': 4, 'E': 1}\n",
      "541: {'E': 5}\n",
      "542: {'B': 4, 'E': 1}\n",
      "543: {'E': 5}\n",
      "544: {'E': 5}\n",
      "545: {'B': 1, 'A': 3, 'E': 1}\n",
      "Moderator Intervention: A\n",
      "546: {'E': 2, 'B': 3}\n",
      "547: {'E': 5}\n",
      "548: {'E': 5}\n",
      "549: {'E': 5}\n",
      "550: {'B': 5}\n",
      "551: {'B': 5}\n",
      "552: {'B': 4, 'E': 1}\n",
      "553: {'E': 5}\n",
      "554: {'B': 4, 'E': 1}\n",
      "555: {'E': 5}\n",
      "556: {'B': 4, 'E': 1}\n",
      "557: {'E': 5}\n",
      "558: {'B': 5}\n",
      "559: {'B': 4, 'E': 1}\n",
      "560: {'B': 5}\n",
      "561: {'E': 5}\n",
      "562: {'E': 5}\n",
      "563: {'B': 4, 'E': 1}\n",
      "564: {'E': 5}\n",
      "565: {'E': 5}\n",
      "566: {'E': 5}\n",
      "567: {'E': 5}\n",
      "568: {'E': 5}\n",
      "569: {'E': 5}\n",
      "570: {'E': 5}\n",
      "571: {'E': 5}\n",
      "572: {'E': 2, 'B': 3}\n",
      "573: {'E': 5}\n",
      "574: {'E': 1, 'B': 4}\n",
      "575: {'E': 5}\n",
      "576: {'B': 5}\n",
      "577: {'E': 4, 'B': 1}\n",
      "578: {'E': 5}\n",
      "579: {'E': 5}\n",
      "580: {'E': 5}\n",
      "581: {'E': 3, 'B': 1, 'V': 1}\n",
      "Moderator Intervention: E\n",
      "582: {'B': 5}\n",
      "583: {'E': 5}\n",
      "584: {'E': 5}\n",
      "585: {'E': 5}\n",
      "586: {'E': 5}\n",
      "587: {'E': 5}\n",
      "588: {'B': 5}\n",
      "589: {'B': 1, 'E': 4}\n",
      "590: {'B': 4, 'E': 1}\n",
      "591: {'E': 4, 'B': 1}\n",
      "592: {'E': 5}\n",
      "593: {'E': 5}\n",
      "594: {'E': 5}\n",
      "595: {'B': 3, 'E': 2}\n",
      "596: {'E': 2, 'B': 3}\n",
      "597: {'B': 2, 'E': 3}\n",
      "598: {'B': 5}\n",
      "599: {'B': 5}\n",
      "600: {'B': 5}\n",
      "601: {'E': 5}\n",
      "602: {'E': 5}\n",
      "603: {'B': 5}\n",
      "604: {'E': 5}\n",
      "605: {'E': 5}\n",
      "606: {'E': 5}\n",
      "607: {'E': 1, 'B': 4}\n",
      "608: {'E': 5}\n",
      "609: {'E': 4, 'B': 1}\n",
      "610: {'B': 5}\n",
      "611: {'E': 4, 'B': 1}\n",
      "612: {'E': 1, 'B': 4}\n",
      "613: {'E': 5}\n",
      "614: {'B': 5}\n",
      "615: {'B': 4, 'E': 1}\n",
      "616: {'B': 4, 'E': 1}\n",
      "617: {'B': 5}\n",
      "618: {'B': 5}\n",
      "619: {'E': 5}\n",
      "620: {'E': 5}\n",
      "621: {'E': 5}\n",
      "622: {'E': 5}\n",
      "623: {'E': 5}\n",
      "624: {'E': 5}\n",
      "625: {'A': 4, 'B': 1}\n",
      "626: {'E': 5}\n",
      "627: {'B': 2, 'E': 3}\n",
      "628: {'A': 4, 'B': 1}\n",
      "629: {'B': 5}\n",
      "630: {'E': 5}\n",
      "631: {'E': 5}\n",
      "632: {'E': 5}\n",
      "633: {'E': 5}\n",
      "634: {'E': 5}\n",
      "635: {'B': 5}\n",
      "636: {'B': 3, 'E': 2}\n",
      "637: {'E': 5}\n",
      "638: {'B': 2, 'E': 3}\n",
      "639: {'E': 2, 'B': 3}\n",
      "640: {'E': 5}\n",
      "641: {'E': 5}\n",
      "642: {'B': 5}\n",
      "643: {'E': 5}\n",
      "644: {'E': 5}\n",
      "645: {'E': 5}\n",
      "646: {'B': 3, 'E': 2}\n",
      "647: {'E': 2, 'B': 3}\n",
      "648: {'B': 5}\n",
      "649: {'B': 4, 'E': 1}\n",
      "650: {'B': 1, 'E': 4}\n",
      "651: {'E': 5}\n",
      "652: {'B': 1, 'A': 3, 'E': 1}\n",
      "Moderator Intervention: A\n",
      "653: {'E': 5}\n",
      "654: {'E': 5}\n",
      "655: {'E': 5}\n",
      "656: {'E': 5}\n",
      "657: {'E': 5}\n",
      "658: {'B': 5}\n",
      "659: {'E': 4, 'B': 1}\n",
      "660: {'B': 5}\n",
      "661: {'E': 5}\n",
      "662: {'E': 5}\n",
      "663: {'B': 2, 'E': 3}\n",
      "664: {'E': 5}\n",
      "665: {'B': 5}\n",
      "666: {'B': 5}\n",
      "667: {'E': 5}\n",
      "668: {'E': 5}\n",
      "669: {'E': 5}\n",
      "670: {'E': 5}\n",
      "671: {'E': 5}\n",
      "672: {'E': 5}\n",
      "673: {'E': 5}\n",
      "674: {'B': 4, 'E': 1}\n",
      "675: {'E': 4, 'B': 1}\n",
      "676: {'E': 5}\n",
      "677: {'E': 5}\n",
      "678: {'E': 2, 'B': 3}\n",
      "679: {'E': 5}\n",
      "680: {'E': 2, 'B': 3}\n",
      "681: {'E': 4, 'V': 1}\n",
      "682: {'E': 5}\n",
      "683: {'B': 4, 'E': 1}\n",
      "684: {'E': 4, 'B': 1}\n",
      "685: {'E': 4, 'B': 1}\n",
      "686: {'E': 4, 'B': 1}\n",
      "687: {'B': 5}\n",
      "688: {'E': 5}\n",
      "689: {'B': 4, 'E': 1}\n",
      "690: {'E': 5}\n",
      "691: {'B': 4, 'E': 1}\n",
      "692: {'B': 3, 'E': 2}\n",
      "693: {'B': 4, 'E': 1}\n",
      "694: {'E': 5}\n",
      "695: {'E': 5}\n",
      "696: {'B': 5}\n",
      "697: {'E': 5}\n",
      "698: {'A': 4, 'B': 1}\n",
      "699: {'B': 4, 'E': 1}\n",
      "700: {'E': 5}\n",
      "701: {'E': 5}\n",
      "702: {'E': 4, 'B': 1}\n",
      "703: {'E': 5}\n",
      "704: {'E': 5}\n",
      "705: {'E': 5}\n",
      "706: {'E': 5}\n",
      "707: {'B': 5}\n",
      "708: {'B': 5}\n",
      "709: {'E': 5}\n",
      "710: {'B': 5}\n",
      "711: {'E': 5}\n",
      "712: {'E': 5}\n",
      "713: {'E': 5}\n",
      "714: {'B': 5}\n",
      "715: {'E': 5}\n",
      "716: {'B': 4, 'E': 1}\n",
      "717: {'E': 5}\n",
      "718: {'B': 4, 'E': 1}\n",
      "719: {'E': 5}\n",
      "720: {'E': 5}\n",
      "721: {'E': 5}\n",
      "722: {'B': 3, 'E': 2}\n",
      "723: {'E': 5}\n",
      "724: {'E': 5}\n",
      "725: {'E': 3, 'B': 2}\n",
      "726: {'B': 3, 'E': 2}\n",
      "727: {'E': 5}\n",
      "728: {'E': 5}\n",
      "729: {'E': 5}\n",
      "730: {'E': 5}\n",
      "731: {'E': 4, 'B': 1}\n",
      "732: {'B': 5}\n",
      "733: {'B': 5}\n",
      "734: {'E': 5}\n",
      "735: {'E': 5}\n",
      "736: {'A': 4, 'E': 1}\n",
      "737: {'E': 5}\n",
      "738: {'E': 5}\n",
      "739: {'E': 5}\n",
      "740: {'E': 5}\n",
      "741: {'E': 5}\n",
      "742: {'E': 5}\n",
      "743: {'E': 5}\n",
      "744: {'E': 5}\n",
      "745: {'E': 5}\n",
      "746: {'E': 5}\n",
      "747: {'E': 5}\n",
      "748: {'E': 5}\n",
      "749: {'E': 4, 'B': 1}\n",
      "750: {'A': 5}\n",
      "751: {'E': 1, 'B': 4}\n",
      "752: {'E': 1, 'B': 4}\n",
      "753: {'E': 4, 'B': 1}\n",
      "754: {'E': 5}\n",
      "755: {'E': 1, 'B': 4}\n",
      "756: {'E': 5}\n",
      "757: {'E': 5}\n",
      "758: {'E': 5}\n",
      "759: {'B': 1, 'E': 4}\n",
      "760: {'E': 5}\n",
      "761: {'E': 3, 'B': 2}\n",
      "762: {'E': 5}\n",
      "763: {'E': 5}\n",
      "764: {'B': 5}\n",
      "765: {'B': 5}\n",
      "766: {'E': 1, 'B': 4}\n",
      "767: {'E': 1, 'B': 4}\n",
      "768: {'E': 5}\n",
      "769: {'E': 5}\n",
      "770: {'E': 4, 'B': 1}\n",
      "771: {'E': 5}\n",
      "772: {'E': 5}\n",
      "773: {'E': 5}\n",
      "774: {'E': 5}\n",
      "775: {'B': 3, 'E': 2}\n",
      "776: {'B': 5}\n",
      "777: {'E': 3, 'B': 2}\n",
      "778: {'E': 5}\n",
      "779: {'E': 5}\n",
      "780: {'E': 3, 'V': 1, 'B': 1}\n",
      "Moderator Intervention: E\n",
      "781: {'E': 2, 'B': 3}\n",
      "782: {'B': 5}\n",
      "783: {'E': 5}\n",
      "784: {'A': 4, 'B': 1}\n",
      "785: {'B': 2, 'E': 3}\n",
      "786: {'B': 5}\n",
      "787: {'E': 5}\n",
      "788: {'E': 2, 'B': 3}\n",
      "789: {'E': 5}\n",
      "790: {'B': 4, 'E': 1}\n",
      "791: {'E': 5}\n",
      "792: {'B': 1, 'E': 4}\n",
      "793: {'E': 4, 'B': 1}\n",
      "794: {'E': 5}\n",
      "795: {'E': 5}\n",
      "796: {'E': 5}\n",
      "797: {'E': 1, 'B': 4}\n",
      "798: {'E': 5}\n",
      "799: {'E': 4, 'B': 1}\n",
      "800: {'E': 4, 'B': 1}\n",
      "801: {'E': 5}\n",
      "802: {'E': 5}\n",
      "803: {'E': 5}\n",
      "804: {'B': 5}\n",
      "805: {'E': 5}\n",
      "806: {'E': 5}\n",
      "807: {'B': 4, 'E': 1}\n",
      "808: {'E': 5}\n",
      "809: {'E': 5}\n",
      "810: {'E': 5}\n",
      "811: {'E': 5}\n",
      "812: {'E': 5}\n",
      "813: {'B': 3, 'E': 2}\n",
      "814: {'E': 5}\n",
      "815: {'B': 5}\n",
      "816: {'B': 3, 'E': 2}\n",
      "817: {'B': 4, 'E': 1}\n",
      "818: {'E': 3, 'B': 2}\n",
      "819: {'E': 5}\n",
      "820: {'E': 5}\n",
      "821: {'B': 5}\n",
      "822: {'E': 5}\n",
      "823: {'E': 3, 'B': 2}\n",
      "824: {'B': 5}\n",
      "825: {'E': 5}\n",
      "826: {'E': 5}\n",
      "827: {'E': 5}\n",
      "828: {'B': 5}\n",
      "829: {'E': 4, 'B': 1}\n",
      "830: {'E': 5}\n",
      "831: {'E': 5}\n",
      "832: {'B': 4, 'E': 1}\n",
      "833: {'E': 5}\n",
      "834: {'E': 5}\n",
      "835: {'E': 5}\n",
      "836: {'V': 1, 'E': 3, 'B': 1}\n",
      "Moderator Intervention: V\n",
      "837: {'E': 4, 'B': 1}\n",
      "838: {'E': 4, 'B': 1}\n",
      "839: {'B': 4, 'E': 1}\n",
      "840: {'B': 4, 'E': 1}\n",
      "841: {'E': 5}\n",
      "842: {'E': 5}\n",
      "843: {'E': 5}\n",
      "844: {'E': 5}\n",
      "845: {'B': 5}\n",
      "846: {'E': 4, 'V': 1}\n",
      "847: {'E': 4, 'B': 1}\n",
      "848: {'E': 5}\n",
      "849: {'E': 4, 'B': 1}\n",
      "850: {'E': 5}\n",
      "851: {'E': 5}\n",
      "852: {'E': 5}\n",
      "853: {'E': 5}\n",
      "854: {'B': 2, 'E': 3}\n",
      "855: {'E': 5}\n",
      "856: {'E': 2, 'B': 3}\n",
      "857: {'E': 4, 'B': 1}\n",
      "858: {'E': 5}\n",
      "859: {'B': 5}\n",
      "860: {'E': 1, 'B': 4}\n",
      "861: {'E': 5}\n",
      "862: {'B': 5}\n",
      "863: {'E': 4, 'B': 1}\n",
      "864: {'E': 5}\n",
      "865: {'E': 5}\n",
      "866: {'E': 4, 'B': 1}\n",
      "867: {'B': 5}\n",
      "868: {'B': 3, 'E': 2}\n",
      "869: {'E': 5}\n",
      "870: {'B': 4, 'E': 1}\n",
      "871: {'B': 4, 'E': 1}\n",
      "872: {'B': 4, 'E': 1}\n",
      "873: {'B': 5}\n",
      "874: {'E': 4, 'B': 1}\n",
      "875: {'E': 4, 'V': 1}\n",
      "876: {'E': 5}\n",
      "877: {'E': 5}\n",
      "878: {'E': 5}\n",
      "879: {'E': 5}\n",
      "880: {'E': 5}\n",
      "881: {'E': 5}\n",
      "882: {'B': 2, 'E': 3}\n",
      "883: {'B': 1, 'E': 4}\n",
      "884: {'E': 5}\n",
      "885: {'B': 3, 'E': 2}\n",
      "886: {'B': 5}\n",
      "887: {'B': 5}\n",
      "888: {'B': 4, 'E': 1}\n",
      "889: {'E': 2, 'B': 3}\n",
      "890: {'E': 5}\n",
      "891: {'E': 5}\n",
      "892: {'E': 5}\n",
      "893: {'E': 5}\n",
      "894: {'E': 4, 'B': 1}\n",
      "895: {'E': 5}\n",
      "896: {'B': 3, 'E': 2}\n",
      "897: {'E': 5}\n",
      "898: {'E': 5}\n",
      "899: {'E': 5}\n",
      "900: {'E': 5}\n",
      "901: {'E': 5}\n",
      "902: {'E': 5}\n",
      "903: {'E': 5}\n",
      "904: {'E': 5}\n",
      "905: {'E': 5}\n",
      "906: {'E': 5}\n",
      "907: {'E': 5}\n",
      "908: {'E': 5}\n",
      "909: {'E': 5}\n",
      "910: {'E': 5}\n",
      "911: {'E': 5}\n",
      "912: {'E': 5}\n",
      "913: {'E': 5}\n",
      "914: {'B': 5}\n",
      "915: {'B': 3, 'E': 2}\n",
      "916: {'E': 3, 'B': 2}\n",
      "917: {'E': 5}\n",
      "918: {'E': 3, 'B': 2}\n",
      "919: {'E': 3, 'B': 2}\n",
      "920: {'E': 5}\n",
      "921: {'E': 5}\n",
      "922: {'E': 4, 'B': 1}\n",
      "923: {'B': 3, 'E': 2}\n",
      "924: {'E': 5}\n",
      "925: {'E': 5}\n",
      "926: {'E': 5}\n",
      "927: {'E': 5}\n",
      "928: {'E': 5}\n",
      "929: {'B': 4, 'E': 1}\n",
      "930: {'B': 3, 'E': 2}\n",
      "931: {'E': 5}\n",
      "932: {'E': 5}\n",
      "933: {'E': 5}\n",
      "934: {'B': 5}\n",
      "935: {'E': 5}\n",
      "936: {'E': 4, 'B': 1}\n",
      "937: {'E': 5}\n",
      "938: {'E': 5}\n",
      "939: {'E': 4, 'A': 1}\n",
      "940: {'E': 5}\n",
      "941: {'E': 4, 'B': 1}\n",
      "942: {'B': 5}\n",
      "943: {'E': 5}\n",
      "944: {'A': 4, 'B': 1}\n",
      "945: {'E': 5}\n",
      "946: {'E': 5}\n",
      "947: {'E': 5}\n",
      "948: {'E': 5}\n",
      "949: {'E': 5}\n",
      "950: {'E': 5}\n",
      "951: {'E': 5}\n",
      "952: {'E': 5}\n",
      "953: {'E': 5}\n",
      "954: {'E': 5}\n",
      "955: {'E': 5}\n",
      "956: {'E': 4, 'B': 1}\n",
      "957: {'B': 3, 'E': 2}\n",
      "958: {'B': 5}\n",
      "959: {'B': 2, 'E': 3}\n",
      "960: {'E': 5}\n",
      "961: {'E': 5}\n",
      "962: {'B': 5}\n",
      "963: {'E': 5}\n",
      "964: {'B': 4, 'E': 1}\n",
      "965: {'B': 5}\n",
      "966: {'E': 5}\n",
      "967: {'B': 1, 'E': 4}\n",
      "968: {'E': 4, 'B': 1}\n",
      "969: {'B': 5}\n",
      "970: {'E': 5}\n",
      "971: {'E': 5}\n",
      "972: {'B': 5}\n",
      "973: {'B': 5}\n",
      "974: {'E': 5}\n",
      "975: {'E': 5}\n",
      "976: {'E': 5}\n",
      "977: {'B': 5}\n",
      "978: {'B': 3, 'E': 2}\n",
      "979: {'E': 5}\n",
      "980: {'E': 5}\n",
      "981: {'E': 5}\n",
      "982: {'E': 5}\n",
      "983: {'B': 5}\n",
      "984: {'E': 5}\n",
      "985: {'B': 3, 'E': 2}\n",
      "986: {'E': 5}\n",
      "987: {'B': 5}\n",
      "988: {'E': 1, 'B': 4}\n",
      "989: {'B': 1, 'E': 4}\n",
      "990: {'A': 5}\n",
      "991: {'E': 5}\n",
      "992: {'E': 5}\n",
      "993: {'B': 4, 'E': 1}\n",
      "994: {'E': 5}\n",
      "995: {'B': 5}\n",
      "996: {'E': 3, 'B': 2}\n",
      "997: {'E': 5}\n",
      "998: {'E': 5}\n",
      "999: {'B': 2, 'A': 2, 'E': 1}\n",
      "Moderator Intervention: A\n",
      "1000: {'E': 5}\n",
      "Validation Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "def create_ensemble(num_models: int, alpha: float, stop_word_proportion: float, validation_proportion: float) -> list[ImprovedNaiveBayes]:\n",
    "    ensemble: list[ImprovedNaiveBayes] = []\n",
    "    for j in range(num_models):\n",
    "        if j == 0:\n",
    "            auto_load = True\n",
    "            valid_prop = validation_proportion\n",
    "        else:\n",
    "            auto_load = False\n",
    "            valid_prop = 0.0\n",
    "        classifier = ImprovedNaiveBayes(\n",
    "            validation_data_split=valid_prop,\n",
    "            alpha=alpha,\n",
    "            stop_word_proportion=stop_word_proportion,\n",
    "            auto_load_data=auto_load\n",
    "        )\n",
    "        ensemble.append(classifier)\n",
    "    data_to_split = ensemble[0].training_data\n",
    "    split_size = len(data_to_split.data) // num_models\n",
    "    split_data = [data_to_split.data[i * split_size: (i + 1) * split_size] for i in range(num_models)]\n",
    "    for i in range(num_models):\n",
    "        new_datamodel = DataModel('')\n",
    "        new_datamodel.set_data(split_data[i])\n",
    "        if i == 0:\n",
    "            ensemble[i].set_data(training_data=new_datamodel)\n",
    "        else:\n",
    "            ensemble[i].set_data(training_data=new_datamodel, testing_data=ensemble[0].testing_data)\n",
    "    return ensemble\n",
    "\n",
    "\n",
    "def test_ensemble(ensemble: list[ImprovedNaiveBayes], fileout: str, moderator_model = None) -> None:\n",
    "    with open(fileout, 'w') as f:\n",
    "        f.write(\"id,class\\n\")\n",
    "        for data in ensemble[0].testing_data.data:\n",
    "            class_counts = {}\n",
    "            for classifier in ensemble:\n",
    "                predicted_class = classifier.classify_abstract(data.abstract)\n",
    "                if predicted_class in class_counts:\n",
    "                    class_counts[predicted_class] += 1\n",
    "                else:\n",
    "                    class_counts[predicted_class] = 1\n",
    "            print(f\"{data.id}: {class_counts}\")\n",
    "            if len(class_counts) >= 3 and moderator_model:\n",
    "                max_class = moderator_model.classify_abstract(data.abstract)\n",
    "                print(f\"Moderator Intervention: {max_class}\")\n",
    "            else:\n",
    "                max_class = max(class_counts, key=class_counts.get)\n",
    "            f.write(f\"{data.id},{max_class}\\n\")\n",
    "\n",
    "\n",
    "def validate_ensemble(ensemble: list[ImprovedNaiveBayes], validation_set: DataModel, moderator = None) -> None:\n",
    "    correct = 0\n",
    "    for data in validation_set.data:\n",
    "        class_counts = {}\n",
    "        for classifier in ensemble:\n",
    "            predicted_class = classifier.classify_abstract(data.abstract)\n",
    "            if predicted_class in class_counts:\n",
    "                class_counts[predicted_class] += 1\n",
    "            else:\n",
    "                class_counts[predicted_class] = 1\n",
    "        if len(class_counts) >= 3 and moderator:\n",
    "            max_class = moderator.classify_abstract(data.abstract)\n",
    "        else:\n",
    "            max_class = max(class_counts, key=class_counts.get)\n",
    "        if max_class == data.class_:\n",
    "            correct += 1\n",
    "    print(f\"Validation Accuracy: {correct / len(validation_set.data)}\")\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # total_accuracy = 0\n",
    "    # num_trials = 1\n",
    "\n",
    "    # for _ in range(num_trials):\n",
    "    #     classifier = ImprovedNaiveBayes(\n",
    "    #         validation_data_split=0.0,\n",
    "    #         alpha = 0.001,\n",
    "    #         stop_word_proportion=0.01\n",
    "    #     )\n",
    "    #     total_accuracy += classifier.get_validation_accuracy()\n",
    "    # print(f\"Average Validation Accuracy: {total_accuracy / num_trials}\")\n",
    "    # classifier.save()\n",
    "    # classifier.run_test_data(\"new_output.csv\", type_=\"test\")\n",
    "\n",
    "\n",
    "    moderator = ImprovedNaiveBayes(\n",
    "        validation_data_split=0.5,\n",
    "        alpha=0.0001,\n",
    "        stop_word_proportion=0.02\n",
    "    )\n",
    "    ensemble: list[ImprovedNaiveBayes] = create_ensemble(5, 0.0001, 0.02, 0.2)\n",
    "    test_ensemble(ensemble, \"ensemble_output.csv\", moderator_model=moderator)\n",
    "    validate_ensemble(ensemble, ensemble[0].validation_data, moderator=moderator)\n",
    "    ensemble[0].save()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
