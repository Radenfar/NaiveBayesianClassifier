{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Naive Bayesian Classification (acom884)\n",
    "## Code:\n",
    "### Figure 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # Random is the ONLY module I am allowing myself to use for this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2. Simple Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, id: int, class_: str | None, abstract: str) -> None:\n",
    "        self.id: int = id\n",
    "        self.class_: str = class_\n",
    "        self.abstract: str = abstract\n",
    "\n",
    "    def get_type(self) -> str:\n",
    "        '''Returns: \"test\" | \"train\"'''\n",
    "        return \"train\" if self.class_ else \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3. DataModel Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModel:\n",
    "    def __init__(self, data_path: str, do_shuffle: bool = False) -> None:\n",
    "        '''\n",
    "        The DataModel class is a container for the data in the dataset.\n",
    "        This will be greatly helpful for the model to access the data in a structured way, and minimise risk of errors.\n",
    "        The data is stored in a list of Data objects, which contain the id, class, and abstract of the data as attributes.\n",
    "        The vocabulary size is also stored as an attribute, which is the number of unique words in the dataset.\n",
    "        This DataModel will be able to split data for validation sets and ensembles.\n",
    "        '''\n",
    "        self.data: list[Data] = []\n",
    "        self.vocabulary_size: int = 0\n",
    "        if not data_path or data_path.strip() == \"\":\n",
    "            self.data = []\n",
    "        else:\n",
    "            with open(data_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            if do_shuffle:\n",
    "                random.shuffle(lines) # shouldn't be used for the test data just in case Kaggle expects the data in order\n",
    "            for line in lines[1:]:\n",
    "                line_split = line.strip().replace('\"', '').split(',')\n",
    "                if len(line_split) == 3:\n",
    "                    id, class_, abstract = line_split\n",
    "                    data = Data(id, class_, abstract)\n",
    "                    self.data.append(data)\n",
    "                else:\n",
    "                    id, abstract = line_split\n",
    "                    data = Data(id, None, abstract)\n",
    "                    self.data.append(data)\n",
    "        self.vocabulary_size = self.get_vocabulary_size()\n",
    "\n",
    "\n",
    "    def set_data(self, data: list[Data]) -> None:\n",
    "        '''Sets the data of this model to the given data and updates the vocab size.'''\n",
    "        self.data = data\n",
    "        self.vocabulary_size = self.get_vocabulary_size()\n",
    "\n",
    "\n",
    "    def get_vocabulary_size(self) -> int:\n",
    "        '''Returns the number of unique words in the dataset.'''\n",
    "        vocabulary = set()\n",
    "        for data in self.data:\n",
    "            words = data.abstract.split()\n",
    "            for word in words:\n",
    "                vocabulary.add(word)\n",
    "        self.vocabulary_size = len(vocabulary)\n",
    "        return self.vocabulary_size\n",
    "\n",
    "\n",
    "    def split_model(self, proportion: float) -> 'DataModel':\n",
    "        '''Splits this dataset into two data sets based on the proportion of the current data to be in the new split.'''\n",
    "        split_data = DataModel('')\n",
    "        split_index = int(len(self.data) * proportion)\n",
    "        split_data.set_data(self.data[:split_index])\n",
    "        self.set_data(self.data[split_index:])\n",
    "        return split_data\n",
    "    \n",
    "    \n",
    "    def eliminate_stop_words(self, stop_word_proportion) -> None:\n",
    "        '''\n",
    "        Stopwords include:\n",
    "        - Any word that is a single character\n",
    "        - Any word that is a number\n",
    "        - The top 10% of words that appear in all classes\n",
    "        '''\n",
    "        word_count = {}\n",
    "        num_eliminated = 0\n",
    "        for data in self.data:\n",
    "            words = data.abstract.split()\n",
    "            new_data = []\n",
    "            for word in words:\n",
    "                if len(word) > 1 and not word.isdigit():\n",
    "                    if word in word_count:\n",
    "                        word_count[word] += 1\n",
    "                    else:\n",
    "                        word_count[word] = 1\n",
    "                    new_data.append(word)\n",
    "                else:\n",
    "                    num_eliminated += 1\n",
    "            data.abstract = ' '.join(new_data)\n",
    "        stop_words = set()\n",
    "        num_words_to_eliminate = int(len(word_count) * stop_word_proportion)\n",
    "        for _ in range(num_words_to_eliminate):\n",
    "            max_word = max(word_count, key=word_count.get)\n",
    "            stop_words.add(max_word)\n",
    "            del word_count[max_word]\n",
    "            num_eliminated += 1\n",
    "        for data in self.data:\n",
    "            words = data.abstract.split()\n",
    "            new_data = []\n",
    "            for word in words:\n",
    "                if word not in stop_words:\n",
    "                    new_data.append(word)\n",
    "            data.abstract = ' '.join(new_data)\n",
    "        self.vocabulary_size = self.get_vocabulary_size()\n",
    "        print(f'Eliminated {num_eliminated} words. Including the {len(stop_words)} most common and {num_eliminated - len(stop_words)} single char / number words.')\n",
    "\n",
    "    \n",
    "    def bootstrap_sample(self, proportion, num_models) -> list['DataModel']:\n",
    "        '''Returns a list of num_models DataModels, each with a random sample of the data with proportion of the original data.'''\n",
    "        num_per_model = int(len(self.data) * proportion)\n",
    "        models = []\n",
    "        for _ in range(num_models):\n",
    "            new_data = []\n",
    "            for _ in range(num_per_model):\n",
    "                new_data.append(random.choice(self.data))\n",
    "            new_model = DataModel('')\n",
    "            new_model.set_data(new_data)\n",
    "            models.append(new_model)\n",
    "        return models        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4. Standard Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardNaiveBayes:\n",
    "    def __init__(self) -> None:\n",
    "        ''''\n",
    "        This is the standard Naive Bayes classifier. \n",
    "        It uses the training data to calculate the probabilities of each word in each class and then uses these probabilities to classify the test data.\n",
    "        The standard Naive Bayes has absolutely no added features or optimizations but still managed to get a 0.80 accuracy on the test data.\n",
    "        '''\n",
    "        self.training_data: DataModel = DataModel(\n",
    "            data_path=r\"data\\trg.csv\",\n",
    "            do_shuffle=False # this counts as data pre-processing and won't be used for the standard model\n",
    "        )\n",
    "        self.testing_data: DataModel = DataModel(\n",
    "            data_path=r\"data\\tst.csv\",\n",
    "            do_shuffle=False # ditto above\n",
    "        )\n",
    "        self.classes = [c for c in set([data.class_ for data in self.training_data.data])]\n",
    "        self.class_counts = self.get_class_counts()\n",
    "        self.class_probabilities = [count / len(self.training_data.data) for count in self.class_counts]\n",
    "        self.word_counts = self.get_word_counts()\n",
    "\n",
    "\n",
    "    def run_test_data(self, fileout: str) -> None:\n",
    "        with open(fileout, 'w') as f:\n",
    "            f.write(\"id,class\\n\")\n",
    "            for data in self.testing_data.data:\n",
    "                f.write(f\"{data.id},{self.classify_abstract(data.abstract)}\\n\")\n",
    "\n",
    "\n",
    "    def get_word_probability(self, word: str, class_index: int) -> float:\n",
    "        '''\n",
    "        p(class|word) = p(word|class) * p(class) / p(word)\n",
    "        '''\n",
    "        word_count = self.word_counts[class_index].get(word, 0)\n",
    "        class_count = self.class_counts[class_index]\n",
    "        word_in_class = word_count / class_count\n",
    "        class_probability = self.class_probabilities[class_index]\n",
    "        word_in_data = sum([self.word_counts[i].get(word, 0) for i in range(len(self.classes))]) / len(self.training_data.data)\n",
    "        if word_in_data == 0:\n",
    "            return 0\n",
    "        return word_in_class * class_probability / word_in_data\n",
    "\n",
    "\n",
    "    def classify_abstract(self, abstract: str) -> str:\n",
    "        '''Classifies the abstract into one of the classes. Returns the class. Uses the Naive Bayesian Classifier algorithm.'''\n",
    "        abstract_words = abstract.split()\n",
    "        class_probabilities = []\n",
    "        for i in range(len(self.classes)):\n",
    "            cur_class_probability = self.class_probabilities[i]\n",
    "            cur_class_probability = 1\n",
    "            for word in abstract_words:\n",
    "                cur_word_probability = self.get_word_probability(word, i)\n",
    "                if cur_word_probability == 0:\n",
    "                    continue\n",
    "                cur_class_probability *= cur_word_probability\n",
    "            class_probabilities.append(cur_class_probability)\n",
    "        max_class = self.classes[class_probabilities.index(max(class_probabilities))]\n",
    "        return max_class\n",
    "\n",
    "\n",
    "    def get_class_counts(self) -> list[int]:\n",
    "        '''Returns the count of each class in the training data. Match the order of the classes with the order of self.classes (classes[i] -> class_counts[i])'''\n",
    "        class_counts = [0] * len(self.classes)\n",
    "        for data in self.training_data.data:\n",
    "            class_counts[self.classes.index(data.class_)] += 1\n",
    "        return class_counts\n",
    "    \n",
    "\n",
    "    def get_word_counts(self) -> list[dict[str, int]]:\n",
    "        '''Returns the count of each word in each class. Match the order of the classes with the order of self.classes (classes[i] -> word_counts[i])'''\n",
    "        word_counts = [{} for _ in range(len(self.classes))]\n",
    "        for data in self.training_data.data:\n",
    "            for word in data.abstract.split():\n",
    "                if word not in word_counts[self.classes.index(data.class_)]:\n",
    "                    word_counts[self.classes.index(data.class_)][word] = 1\n",
    "                else:\n",
    "                    word_counts[self.classes.index(data.class_)][word] += 1\n",
    "        return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5. Main Routine for the Standard Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 10-fold repeated Cross-Validation for the Evaluation metric.\n",
    "    total_accuracy = 0\n",
    "    for _ in range(10):\n",
    "        accuracy = 0\n",
    "        model = StandardNaiveBayes()\n",
    "        validation_data = model.training_data.split_model(0.1)\n",
    "        for data in validation_data.data:\n",
    "            classification = model.classify_abstract(data.abstract)\n",
    "            if classification == data.class_:\n",
    "                accuracy += 1\n",
    "        total_accuracy += accuracy / len(validation_data.data)\n",
    "    print(f\"Average accuracy: {total_accuracy / 10}\")\n",
    "        \n",
    "    # Simple run of the model on test data\n",
    "    classifier = StandardNaiveBayes()\n",
    "    classifier.run_test_data(fileout=\"standard_output.csv\")\n",
    "    '''\n",
    "    -->    Accuracy Obtained: 0.800 (Kaggle)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 6. N-Gram Feature Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class N_Gram_DataModel:\n",
    "    def __init__(self, data_path: str, do_shuffle: bool = False) -> None:\n",
    "        '''\n",
    "        Difference between MN_DataModel and DataModel is that it will combine multiple words as features.\n",
    "        This means that the vocabulary size will be smaller as words are paired together.\n",
    "        This could capture the meaning of the words better, but also results in less features and potentially overfitting.\n",
    "        '''\n",
    "        self.data: list[Data] = []\n",
    "        self.vocabulary_size: int = 0\n",
    "        if not data_path or data_path.strip() == \"\":\n",
    "            self.data = []\n",
    "        else:\n",
    "            with open(data_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            if do_shuffle:\n",
    "                random.shuffle(lines) # shouldn't be used for the test data just in case Kaggle expects the data in order\n",
    "            for line in lines[1:]:\n",
    "                line_split = line.strip().replace('\"', '').split(',')\n",
    "                if len(line_split) == 3:\n",
    "                    id, class_, abstract = line_split\n",
    "                    data = Data(id, class_, abstract)\n",
    "                    self.data.append(data)\n",
    "                else:\n",
    "                    id, abstract = line_split\n",
    "                    data = Data(id, None, abstract)\n",
    "                    self.data.append(data)\n",
    "        self.vocabulary_size = self.get_vocabulary_size()\n",
    "\n",
    "\n",
    "    def set_data(self, data: list[Data]) -> None:\n",
    "        '''Simplt sets the data and ensures to update the vocab size attribute.'''\n",
    "        self.data = data\n",
    "        self.vocabulary_size = self.get_vocabulary_size()\n",
    "\n",
    "\n",
    "    def get_vocabulary_size(self) -> int:\n",
    "        '''Returns the number of unique words in the dataset.'''\n",
    "        vocabulary = set()\n",
    "        for data in self.data:\n",
    "            words = self.split_abstract(data.abstract)\n",
    "            for word in words:\n",
    "                vocabulary.add(word)\n",
    "        self.vocabulary_size = len(vocabulary)\n",
    "        return self.vocabulary_size\n",
    "\n",
    "\n",
    "    def split_model(self, proportion: float) -> 'N_Gram_DataModel':\n",
    "        '''Splits this dataset into two data sets based on the proportion of the current data to be in the new split.'''\n",
    "        split_data = N_Gram_DataModel('')\n",
    "        split_index = int(len(self.data) * proportion)\n",
    "        split_data.set_data(self.data[:split_index])\n",
    "        self.set_data(self.data[split_index:])\n",
    "        return split_data\n",
    "    \n",
    "\n",
    "    def split_abstract(self, abstract: str) -> list[str]:\n",
    "        '''Splits the abstract into a list of paired words.'''\n",
    "        words = abstract.split()\n",
    "        combined_words = []\n",
    "        for i in range(0, len(words), 2):\n",
    "            if i + 1 < len(words):\n",
    "                combined_words.append(words[i] + ' ' + words[i + 1])\n",
    "            else:\n",
    "                combined_words.append(words[i])\n",
    "        return combined_words\n",
    "\n",
    "\n",
    "    def eliminate_stop_words(self, stop_word_proportion) -> None:\n",
    "        '''\n",
    "        Stopwords include:\n",
    "        - Any word that is a single character\n",
    "        - Any word that is a number\n",
    "        - The top n% of words to appear\n",
    "        '''\n",
    "        # first, get the single char / number words out\n",
    "        word_count = {}\n",
    "        num_eliminated = 0\n",
    "        for data in self.data:\n",
    "            words = self.split_abstract(data.abstract)\n",
    "            new_data = []\n",
    "            for word in words:\n",
    "                if len(word) > 1 and not word.isdigit():\n",
    "                    if word in word_count:\n",
    "                        word_count[word] += 1\n",
    "                    else:\n",
    "                        word_count[word] = 1\n",
    "                    new_data.append(word)\n",
    "                else:\n",
    "                    num_eliminated += 1\n",
    "            data.abstract = ' '.join(new_data)\n",
    "        # now eliminate the top (stop_word_proportion)% of words\n",
    "        stop_words = set()\n",
    "        num_words_to_eliminate = int(len(word_count) * stop_word_proportion)\n",
    "        for _ in range(num_words_to_eliminate):\n",
    "            max_word = max(word_count, key=word_count.get)\n",
    "            stop_words.add(max_word)\n",
    "            del word_count[max_word]\n",
    "            num_eliminated += 1\n",
    "        for data in self.data:\n",
    "            words = self.split_abstract(data.abstract)\n",
    "            new_data = []\n",
    "            for word in words:\n",
    "                if word not in stop_words:\n",
    "                    new_data.append(word)\n",
    "            data.abstract = ' '.join(new_data)\n",
    "        self.vocabulary_size = self.get_vocabulary_size()\n",
    "        print(f'Eliminated {num_eliminated} words. Including the {len(stop_words)} most common and {num_eliminated - len(stop_words)} single char / number words.')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 7. Improved Naive Bayesian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedNaiveBayes:\n",
    "    def __init__(self, validation_data_split: float, alpha: float, stop_word_proportion: float, auto_load_data: bool = True) -> None:\n",
    "        '''\n",
    "        This model is an improvement on the standard naive bayes model. It includes the following improvements:\n",
    "        - Supporting validation data splits\n",
    "        - Dirichlet smoothing\n",
    "        - Stop word elimination\n",
    "        - Ability to combine words as features\n",
    "        - Supporting manual data loading for ensembles / superstructures of models        \n",
    "        '''\n",
    "        if auto_load_data:\n",
    "            self.training_data: DataModel = DataModel(\n",
    "                data_path=r\"data\\trg.csv\",\n",
    "                do_shuffle=True                         # Data stochasiticity parameter Figure 7.1\n",
    "            )\n",
    "            self.testing_data: DataModel = DataModel(\n",
    "                data_path=r\"data\\tst.csv\",\n",
    "                do_shuffle=False\n",
    "            )\n",
    "            self.training_data.eliminate_stop_words(stop_word_proportion) # stop word elimination parameter Figure 7.2\n",
    "            if validation_data_split > 0.0:        # validation data split parameter Figure 7.3\n",
    "                self.validation_data: DataModel = self.training_data.split_model(validation_data_split)\n",
    "            else:\n",
    "                self.validation_data = None\n",
    "            self.alpha = alpha\n",
    "            self.vocab_size = self.training_data.vocabulary_size\n",
    "            self.classes = [c for c in set([data.class_ for data in self.training_data.data])]\n",
    "            self.class_counts = self.get_class_counts()\n",
    "            self.class_probabilities = [count / len(self.training_data.data) for count in self.class_counts]\n",
    "            self.word_counts = self.get_word_counts()\n",
    "        else:\n",
    "            self.training_data = DataModel('')\n",
    "            self.testing_data = DataModel('')\n",
    "            self.validation_data = None\n",
    "            self.alpha = alpha                      # Dirichlet smoothing hyperparameter Figure 7.4\n",
    "            self.vocab_size = 0                     # Dirichlet smoothing hyperparameter Figure 7.5\n",
    "            self.classes = []\n",
    "            self.class_counts = []\n",
    "            self.class_probabilities = []\n",
    "            self.word_counts = []\n",
    "\n",
    "\n",
    "    def set_data(self, training_data: DataModel = None, testing_data: DataModel = None, validation_data: DataModel = None) -> None:\n",
    "        '''Method for manually setting the data. Needs to update several attributes of the model.'''\n",
    "        if training_data:\n",
    "            self.training_data.set_data(training_data.data)\n",
    "        if testing_data:\n",
    "            self.testing_data.set_data(testing_data.data)\n",
    "        if validation_data:\n",
    "            if self.validation_data:\n",
    "                self.validation_data.set_data(validation_data.data)\n",
    "            else:\n",
    "                self.validation_data = DataModel('')\n",
    "                self.validation_data.set_data(validation_data.data)\n",
    "        self.vocab_size = self.training_data.vocabulary_size\n",
    "        self.classes = [c for c in set([data.class_ for data in self.training_data.data])]\n",
    "        self.class_counts = self.get_class_counts()\n",
    "        self.class_probabilities = [count / len(self.training_data.data) for count in self.class_counts]\n",
    "        self.word_counts = self.get_word_counts()\n",
    "\n",
    "\n",
    "    def get_validation_accuracy(self, print_prediciton: bool = False) -> float:\n",
    "        '''Returns the accuracy of the model on the validation data.'''\n",
    "        if not self.validation_data:\n",
    "            print(f\"Model has no validation set.\")\n",
    "            return 0\n",
    "        correct = 0\n",
    "        for data in self.validation_data.data:\n",
    "            predicted_class = self.classify_abstract(data.abstract)\n",
    "            if print_prediciton:\n",
    "                print(f\"Predicted: {predicted_class} | Actual: {data.class_}\")\n",
    "            if predicted_class == data.class_:\n",
    "                correct += 1\n",
    "        return correct / len(self.validation_data.data)\n",
    "\n",
    "\n",
    "    def run_test_data(self, fileout: str, type_: str = \"test\") -> None:\n",
    "        '''Runs the model on either the testing or training data and writes the results to a file.'''\n",
    "        with open(fileout, 'w') as f:\n",
    "            f.write(\"id,class\\n\")\n",
    "            if type_ == \"test\":\n",
    "                for data in self.testing_data.data:\n",
    "                    f.write(f\"{data.id},{self.classify_abstract(data.abstract)}\\n\")\n",
    "            elif type_ == \"train\":\n",
    "                for data in self.training_data.data:\n",
    "                    f.write(f\"{data.id},{self.classify_abstract(data.abstract)}\\n\")\n",
    "            else:\n",
    "                raise ValueError(\"Invalid type_ argument. Must be 'test', 'validation', or 'train'.\")\n",
    "    \n",
    "    \n",
    "    def get_word_probability(self, word: str, class_index: int) -> float:\n",
    "        '''p(class|word) = p(word|class) * p(class) / p(word) ++ Dirichlet smoothing'''\n",
    "        word_count = self.word_counts[class_index].get(word, 0)\n",
    "        class_count = self.class_counts[class_index]\n",
    "        word_in_class = (word_count + self.alpha) / (class_count + self.alpha * self.vocab_size)\n",
    "        class_probability = self.class_probabilities[class_index]\n",
    "        word_in_data = (sum([self.word_counts[i].get(word, 0) for i in range(len(self.classes))]) + self.alpha) / (len(self.training_data.data) + self.alpha * self.vocab_size)\n",
    "        return word_in_class * class_probability / word_in_data\n",
    "\n",
    "\n",
    "    def classify_abstract(self, abstract: str) -> str:\n",
    "        '''Classifies the abstract into one of the classes. Returns the class. Uses the Naive Bayesian Classifier algorithm.'''\n",
    "        abstract_words = abstract.split()\n",
    "        class_probabilities = []\n",
    "        for i in range(len(self.classes)):\n",
    "            cur_class_probability = self.class_probabilities[i]\n",
    "            cur_class_probability = 1\n",
    "            for word in abstract_words:\n",
    "                cur_word_probability = self.get_word_probability(word, i)\n",
    "                if cur_word_probability == 0:\n",
    "                    continue\n",
    "                cur_class_probability *= cur_word_probability\n",
    "            class_probabilities.append(cur_class_probability)\n",
    "        max_class = self.classes[class_probabilities.index(max(class_probabilities))]\n",
    "        return max_class\n",
    "    \n",
    "\n",
    "\n",
    "    def get_class_counts(self) -> list[int]:\n",
    "        '''Returns the count of each class in the training data. Match the order of the classes with the order of self.classes (classes[i] -> class_counts[i])'''\n",
    "        class_counts = [0] * len(self.classes)\n",
    "        for data in self.training_data.data:\n",
    "            class_counts[self.classes.index(data.class_)] += 1\n",
    "        return class_counts\n",
    "    \n",
    "\n",
    "    def get_word_counts(self) -> list[dict[str, int]]:\n",
    "        '''Returns the count of each word in each class. Matches the order of the classes with the order of self.classes (classes[i] -> word_counts[i])'''\n",
    "        word_counts = [{} for _ in range(len(self.classes))]\n",
    "        for data in self.training_data.data:\n",
    "            words = data.abstract.split()\n",
    "            for word in words:\n",
    "                if word not in word_counts[self.classes.index(data.class_)]:\n",
    "                    word_counts[self.classes.index(data.class_)][word] = 1\n",
    "                else:\n",
    "                    word_counts[self.classes.index(data.class_)][word] += 1\n",
    "        return word_counts\n",
    "    \n",
    "\n",
    "    def save(self):\n",
    "        '''Saves the word counts to a txt file'''\n",
    "        with open(\"word_counts.txt\", 'w') as f:\n",
    "            for i in range(len(self.classes)):\n",
    "                f.write('-'*100 + '\\n')\n",
    "                f.write(f\"Class: {self.classes[i]}\\n\")\n",
    "                f.write(f\"Class Count: {self.class_counts[i]}\\n\")\n",
    "                f.write(f\"Class Probability: {self.class_probabilities[i]}\\n\\n\")\n",
    "                for word, count in self.word_counts[i].items():\n",
    "                    f.write(f\"{word}: {count}\\n\")\n",
    "    \n",
    "\n",
    "    def tune_hyper_param(self, min_value: float, max_value: float, step: float, param: str) -> float: # Tuning Method Figure 7.6\n",
    "        '''Tunes the hyperparameter of the model and returns the best value.'''\n",
    "        if not self.validation_data:\n",
    "            print(\"Model has no validation set.\")\n",
    "            return 0\n",
    "        multiplier = 1\n",
    "        if isinstance(min_value, float):\n",
    "            multiplier = 10 ** len(str(min_value).split('.')[1])\n",
    "        min_value = int(min_value * multiplier)\n",
    "        max_value = int(max_value * multiplier)\n",
    "        step = int(step * multiplier)\n",
    "        best_value = min_value\n",
    "        best_accuracy = 0\n",
    "        for value in range(min_value, max_value, step):\n",
    "            setattr(self, param, value)\n",
    "            accuracy = self.get_validation_accuracy()\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_value = value\n",
    "        return best_value / multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 8. Main Routine for Improved Naive Bayesian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminated 41171 words. Including the 602 most common and 40569 single char / number words.\n",
      "Accuracy: 0.9125\n"
     ]
    }
   ],
   "source": [
    "def create_divided_ensemble(num_models: int, alpha: float, stop_word_proportion: float, validation_proportion: float) -> list[ImprovedNaiveBayes]:\n",
    "    '''Creates num_models models and splits the training data into num_models parts. Each model is trained on a different part of the data.'''\n",
    "    ensemble: list[ImprovedNaiveBayes] = []\n",
    "    for j in range(num_models):\n",
    "        if j == 0:\n",
    "            auto_load = True\n",
    "            valid_prop = validation_proportion\n",
    "        else:\n",
    "            auto_load = False\n",
    "            valid_prop = 0.0\n",
    "        classifier = ImprovedNaiveBayes(\n",
    "            validation_data_split=valid_prop,\n",
    "            alpha=alpha,\n",
    "            stop_word_proportion=stop_word_proportion,\n",
    "            auto_load_data=auto_load\n",
    "        )\n",
    "        ensemble.append(classifier)\n",
    "    data_to_split = ensemble[0].training_data\n",
    "    split_size = len(data_to_split.data) // num_models\n",
    "    split_data = [data_to_split.data[i * split_size: (i + 1) * split_size] for i in range(num_models)]\n",
    "    for i in range(num_models):\n",
    "        new_datamodel = DataModel('')\n",
    "        new_datamodel.set_data(split_data[i])\n",
    "        if i == 0:\n",
    "            ensemble[i].set_data(training_data=new_datamodel)\n",
    "        else:\n",
    "            ensemble[i].set_data(training_data=new_datamodel, testing_data=ensemble[0].testing_data)\n",
    "    return ensemble\n",
    "\n",
    "\n",
    "def create_bootstrap_ensemble(num_models: int, alpha: float, stop_word_proportion: float, validation_proportion: float) -> list[ImprovedNaiveBayes]:\n",
    "    '''Creates num_models models and splits the training data into num_models parts. Each model is trained on a different part of the data.'''\n",
    "    ensemble: list[ImprovedNaiveBayes] = []\n",
    "    base_classifier = ImprovedNaiveBayes(\n",
    "        validation_data_split=validation_proportion,\n",
    "        alpha=alpha,\n",
    "        stop_word_proportion=stop_word_proportion\n",
    "    )\n",
    "    data_to_split = base_classifier.training_data\n",
    "    split_data = data_to_split.bootstrap_sample(1/num_models, num_models)\n",
    "    for i in range(num_models):\n",
    "        new_datamodel = DataModel('')\n",
    "        new_datamodel.set_data(split_data[i].data)\n",
    "        new_classifier = ImprovedNaiveBayes(\n",
    "            validation_data_split=0.0,\n",
    "            alpha=alpha,\n",
    "            stop_word_proportion=stop_word_proportion,\n",
    "            auto_load_data=False\n",
    "        )\n",
    "        new_classifier.set_data(training_data=new_datamodel, testing_data=base_classifier.testing_data)\n",
    "        ensemble.append(new_classifier)\n",
    "    return ensemble\n",
    "\n",
    "\n",
    "def test_ensemble(ensemble: list[ImprovedNaiveBayes], fileout: str, moderator_model = None) -> None:\n",
    "    '''Tests the ensemble on the testing data and writes the results to a file.'''\n",
    "    with open(fileout, 'w') as f:\n",
    "        f.write(\"id,class\\n\")\n",
    "        for data in ensemble[0].testing_data.data:\n",
    "            class_counts = {}\n",
    "            for classifier in ensemble:\n",
    "                predicted_class = classifier.classify_abstract(data.abstract)\n",
    "                if predicted_class in class_counts:\n",
    "                    class_counts[predicted_class] += 1\n",
    "                else:\n",
    "                    class_counts[predicted_class] = 1\n",
    "            print(f\"{data.id}: {class_counts}\")\n",
    "            if len(class_counts) >= 3 and moderator_model:\n",
    "                max_class = moderator_model.classify_abstract(data.abstract)\n",
    "                print(f\"Moderator Intervention: {max_class}\")\n",
    "            else:\n",
    "                max_class = max(class_counts, key=class_counts.get)\n",
    "            f.write(f\"{data.id},{max_class}\\n\")\n",
    "\n",
    "\n",
    "def validate_ensemble(ensemble: list[ImprovedNaiveBayes], validation_set: DataModel, moderator: ImprovedNaiveBayes | None = None) -> None:\n",
    "    '''Validates the ensemble on the validation data and prints the accuracy. Uses a moderator model if provided.'''\n",
    "    true_positives = 0\n",
    "    for data in validation_set.data:\n",
    "        class_counts = {}\n",
    "        for classifier in ensemble:\n",
    "            predicted_class = classifier.classify_abstract(data.abstract)\n",
    "            if predicted_class in class_counts:\n",
    "                class_counts[predicted_class] += 1\n",
    "            else:\n",
    "                class_counts[predicted_class] = 1\n",
    "        if len(class_counts) >= 3 and moderator:\n",
    "            max_class = moderator.classify_abstract(data.abstract)\n",
    "        else:\n",
    "            max_class = max(class_counts, key=class_counts.get)\n",
    "        if max_class == data.class_:\n",
    "            true_positives += 1\n",
    "    return true_positives / len(validation_set.data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ensemble: list[ImprovedNaiveBayes] = create_divided_ensemble(\n",
    "        num_models=5,\n",
    "        alpha=0.0001,\n",
    "        stop_word_proportion=0.02,\n",
    "        validation_proportion=0.1\n",
    "    )\n",
    "    print(f\"Accuracy: {validate_ensemble(ensemble, ensemble[0].validation_data)}\")\n",
    "    '''\n",
    "    -->    Accuracy Obtained: 0.920 (Kaggle)\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "#### 1. Data Representation\n",
    "I chose to write in object-oriented and type explicit python. This served two purposes; for one, the assignment would be marked on readability and for another, debugging OOP code can be much simpler. With this in mind, I created a ‘class hierarchy’ which would take advantage of this type of programming. 3 tiers of class would be at play:\n",
    "1.\tA Data class, which represents a single line of data with an id, class and abstract.\n",
    "2.\tA DataModel class which acts as a container for the Data classes and enables safer extraction and manipulation of the datasets.\n",
    "3.\tThe Model class which holds the DataModels as its sets (train, test, valid) and holds the word frequency tables.\n",
    "\n",
    "The SNB I was building was non-time constrained and would require plenty of testing, data splitting and code reuse and this highly modular approach seemed perfect for the task.</br>\n",
    "I decided to completely minimalize my initial Standard Naïve Bayes Classifier and not include any data pre-processing or optimisations, as this would allow me to include these as extensions and see the accuracy differences when creating my improved model. My final model pre-processes with randomisation of the training data. This is because my final model was a majority-choice ensemble and I wanted to randomise the splits of training data between the ensembled models for more accurate validation results.\n",
    "\n",
    "\n",
    "#### 2. My Standard Naive Bayesian Classifier\n",
    "My SNB, the basic Naïve Bayes was intended to be a minimum-viable product of a Naïve Bayesian Classifier, so that the effects of improvements were fully captured. My SNB *[Figure 4]* works by immediately identifying the word and class counts and frequencies once the class object is initialised. Then, the model can be run with the run_test_data method and save the output to a csv.\n",
    "Each classification is a simplistic implementation of Bayes Formula and this approach achieved a 0.800 accuracy on the Kaggle test set.\n",
    "\n",
    "### 3. Implemented Extensions\n",
    "For my improved classifier, I made and tested the following improvements. Not all of these improvements remained however, as will be discussed.</br>\n",
    "\n",
    "1.\tSmoothing</br>\n",
    "In the training data given, classes ‘E’ and ‘B’ made up 94% of the training data, meaning identifying meaningful features for ‘A’ and ‘V’ was going to be difficult. With this in mind I implemented smoothing with a dirichlet distribution. The idea was that this would make more accurate classifications for rare  / unseen words by using prior knowledge of word frequencies. The assumption was that rarer words were more likely to be associated with the classes for which less training data exists. </br>\n",
    "\n",
    "2.\tStop-word analysis</br>\n",
    "Another issue was the existence of words which held no actual value to unseen data. This included numbers, single character words and words that appeared so ubiquitously across the classes that they just biased the model based on class frequency. To fix this I implemented a stop-word system whereby these features (class ubiquity, numbers and single characters) were removed. I also wrote a hyperparameter tuning function for the model and had it iterate over values between (0.01 and 10), before settling on an optimisation of the top 2% of words being removed (retested for the ensemble later). *[Figure 7.2]*</br>\n",
    "\n",
    "3.\tN-gram words</br>\n",
    "I experimented with n-gram words for this classifier. The idea was that combining words together would better capture their meaning in the data and better identify classes with lower training frequency. However, because of the limited size of the training data I actually found this to make my model overfit the training data, and I removed it from my final model. It’s implementation can still be read in *[Figure 6]*.</br>\n",
    "\n",
    "4.\tValidation Splits / Data splits in general</br>\n",
    "As I excluded any data pre-processing in the SNB, I implemented data splitting into the DataModel class so that improved models could split of proportions of their data for validation set testing or for more complex ensemble data distribution methods *[Figure 7.2]*. Additionally, I also implemented randomness into the data splitting, so that techniques such as bagging might be used. *[Figure 7.1]*</br>\n",
    "\n",
    "5.\tTuning of hyperparameters (range-iter averaging)</br>\n",
    "As mentioned in (b), the final improved classifier has a hyperparameter tuning method for which hyperparameters of the models were derived. These figures, such as for alpha, stop_word elimination proportion and so on, proved their effectiveness with big upticks in accuracy. *[Figure 7.6]*</br>\n",
    "\n",
    "6.\tEnsemble Structure</br>\n",
    "Finally, I wanted to test an ensemble structure. My reasoning here was that the size of the training data would inevitably lead to some form of overfitting. A way to mitigate this would be to create an ensemble of classifiers, each with distinct training data. This way, overfitting would increase the difference between the models in the ensemble and help mitigate overall overfitting of the model. The distinct training data and tendency to overfit lead to a strong ensemble structure as the independent models were distinct.</br>\n",
    "\n",
    "### 4. Evaluation\n",
    "My final model consisted of an ensemble of 5 Improved Naïve Bayesian Classifiers, each containing a bag-of-words approach trained on distinct subsets of the training data. Each has had the top 2% of most common words removed as well as has a smoothing alpha of 0.0001 for Dirichlet smoothing as per the best outcomes of my hyperparameter tuning method. This led to a Kaggle accuracy of 0.92, with a 10-iteration validation test result of 0.91.\n",
    "\n",
    "Comparing the improved Bayesian classifier with the original SNB classifier and with a non-ensembled bag-of-words Improved Classifier standing along:\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th>Standard Naïve Bayes</th>\n",
    "      <th>Improved Single</th>\n",
    "      <th>Improved Ensemble</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>10-fold repeat Cross-Validation Accuracy</td>\n",
    "      <td>0.785</td>\n",
    "      <td>0.93</td>\n",
    "      <td>0.910</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Test Accuracy</td>\n",
    "      <td>0.800</td>\n",
    "      <td>0.853</td>\n",
    "      <td>0.920</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "** In all cases of Cross-Validation 10% of the total data was used for validation\n",
    "\n",
    "As evidenced by these results on the test set, there was a steady improvement in accuracy on unseen data at each major stage of my model. However, the improved single model showed significant potential overfitting when run on the test data. This suggests that this individual model is highly overfit to the training data and was the reason I decided to test an ensemble structure.\n",
    "\n",
    "Additionally, I also noted down the average accuracy of models before and after certain landmark implementations:\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Improvement</th>\n",
    "      <th>Accuracy difference (10-fold)</th>\n",
    "      <th>Notes</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Smoothing</td>\n",
    "      <td>-0.014 [0.786]</td>\n",
    "      <td>This was because my alpha parameter was far too large.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Hyperparameter Tuning (alpha tuning)</td>\n",
    "      <td>+0.067 [0.853]</td>\n",
    "      <td>This rapid boost was the effect of tuning on alpha for smoothing.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Stop-words</td>\n",
    "      <td>+0.040 [0.893]</td>\n",
    "      <td>Tuned to 2%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>N-Grams</td>\n",
    "      <td>-0.013 [~0.880]</td>\n",
    "      <td>This idea was scratched **</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Ensemble Structure</td>\n",
    "      <td>+0.027 [0.920]</td>\n",
    "      <td>Final Test Accuracy</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "** As discussed earlier, the N-Grams failure was likely due to massive overfitting as my method involved reducing the size of the resultant training set to around half the size.\n",
    "\n",
    "__Conclusions__:\n",
    "- Each individual improvement included in the final classifier made improvements to the accuracy in both the validation and test sets\n",
    "- The initial model was being greatly affected by words that held no meaningful classification value like 'a', 'if' or '42'.\n",
    "- Improvements such as smoothing can appear to be decreasing performance if hyperparameters haven't been properly optimised\n",
    "- The individual improved model tends to overfit with a large difference between the validation data and the test data but ... \n",
    "- Using this overfitting characteristic as individual members of an ensemble greatly increased the overall accuracy while ensembles can do a good job of reducing overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
