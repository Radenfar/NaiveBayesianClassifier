Step 1: Multinomial Naive Bayes
Multinomial Naive Bayes models the distribution of words in a document as a multinomial. A document is treated as a sequence of words and it is assumed that each word position is generated independently of every other. For classification, we assume that there are a fixed number of classes, c ∈ {1, 2, . . . , m}, each with a fixed set of multinomial parameters. The parameter vector for a class c is ~θc = {θc1, θc2, . . . , θcn}, where n is the size of the vocabulary, P i θci = 1 and θci is the probability that word i occurs in that class. The likelihood of a document is a product of the parameters of the words that appear in the document, p(d| ~θc) = ( P i fi)! Q i fi ! Y i (θci) fi , (1) where fi is the frequency count of word i in document d. By assigning a prior distribution over the set of classes, p( ~θc), we can arrive at the minimum-error classification rule (Duda & Hart, 1973) which selects the class with the largest posterior probability, l(d) = argmaxc " log p( ~θc) +X i fi log θci# , (2) = argmaxc " bc + X i fiwci# , (3) where bc is the threshold term and wci is the class c weight for word i. These values are natural parameters for the decision boundary. This is especially easy to see for binary classification, where the boundary is defined by setting the differences between the positive and negative class parameters equal to zero, (b+ − b−) +X i fi (w+i − w−i) = 0. The form of this equation is identical to the decision boundary learned by the (linear) Support Vector Machine, logistic regression, linear least squares and the perceptron. Naive Bayes’ relatively poor performance results from how it chooses the bc and wci. For the problem of classification, the number of classes and labeled training data for each class is given, but the parameters for each class are not. Parameters must be estimated from the training data. We do this by selecting a Dirichlet prior and taking the expectation of the parameter with respect to the posterior. For details, we refer the reader to Section 2 of Heckerman (1995). Thi  gives us a simple form for the estimate of the multinomial parameter, which involves the number of times word i appears in the documents in class c (Nci), divided by the total number of word occurrences in class c (Nc). For word i, a prior adds in αi imagined occurrences so that the estimate is a smoothed version of the maximum likelihood estimate, ˆθci = Nci + αi Nc + α , (4) where α denotes the sum of the αi . While αi can be set differently for each word, we follow common practice by setting αi = 1 for all words. Substituting the true parameters in Equation 2 with our estimates, we get the MNB classifier, lMNB(d) = argmaxc " log ˆp(θc) +X i fi log Nci + αi Nc + α # , where ˆp(θc) is the class prior estimate. The prior class probabilities, p(θc), could be estimated like the word estimates. However, the class probabilities tend to be overpowered by the combination of word probabilities, so we use a uniform prior estimate for simplicity. The weights for the decision boundary defined by the MNB classifier are  he log parameter estimates, wˆci = log ˆθci.  